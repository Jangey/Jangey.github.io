{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"themes/mytheme/source/CNAME","path":"CNAME","modified":0,"renderable":1},{"_id":"themes/mytheme/source/css/styles.css","path":"css/styles.css","modified":0,"renderable":1},{"_id":"themes/mytheme/source/images/wechatID.JPG","path":"images/wechatID.JPG","modified":0,"renderable":1},{"_id":"source/photography/photos/snow.jpg","path":"photography/photos/snow.jpg","modified":0,"renderable":0},{"_id":"source/photography/photos/pisatower.jpg","path":"photography/photos/pisatower.jpg","modified":0,"renderable":0},{"_id":"source/photography/photos/eiffeltower.jpg","path":"photography/photos/eiffeltower.jpg","modified":0,"renderable":0},{"_id":"source/photography/photos/wall.jpg","path":"photography/photos/wall.jpg","modified":0,"renderable":0},{"_id":"source/photography/photos/jw.jpg","path":"photography/photos/jw.jpg","modified":0,"renderable":0},{"_id":"source/photography/photos/bw.jpg","path":"photography/photos/bw.jpg","modified":0,"renderable":0}],"Cache":[{"_id":"themes/mytheme/_config.yml","hash":"bcbf071d68eb3cedf736fdc7bde76ba463781f5c","modified":1597428881923},{"_id":"source/_posts/backpropagation.md","hash":"edbea9d6d902d92982b92c8d1c906b127f163137","modified":1597428881825},{"_id":"source/_posts/begin-machine-learning.md","hash":"3f7cf9e8a779c8ef8e758223ceaac98240ca7482","modified":1597428881825},{"_id":"source/_posts/decision-tree.md","hash":"54766deb3378dc6116d69042e8d309f70078358f","modified":1597428881825},{"_id":"source/_posts/joker.md","hash":"392716bf9c6de48512b8f7b2959f2fe8a3f5f595","modified":1597428881828},{"_id":"source/_posts/linear-regression.md","hash":"839ff697af99958cd879b70ee9b02bf6d7be33d7","modified":1597428881877},{"_id":"source/_posts/logistic-regression.md","hash":"91ae88cb4f88c0055c33d93ed317ec88e764acbc","modified":1597428881877},{"_id":"source/_posts/neural-network.md","hash":"1a343165c8e29d23c44c54416a238202ed3918ad","modified":1597428881878},{"_id":"source/_posts/old-melody.md","hash":"18611eb5719eaa4897e8374e783dec934d3f1a1b","modified":1597428881883},{"_id":"source/_posts/ten-years.md","hash":"c1efc4f7dba4c67d2a35ad3d47e372a0e55eb05e","modified":1597440393784},{"_id":"source/_posts/notre-dame-cathedral.md","hash":"eac096dccded9014405673114718aa329b758825","modified":1597428881879},{"_id":"source/_posts/random-forest.md","hash":"f580482b65861a1b53acd8e46e7504fdb8b21a9b","modified":1597428881883},{"_id":"source/_posts/untitle.md","hash":"b2b019a880d13a45f9f60d49b4cb5f60e28957c8","modified":1597428881884},{"_id":"source/about/index.md","hash":"2fbde3bf54cf970016fd6ff5df1d2bb98de34605","modified":1597428881885},{"_id":"source/categories/index.md","hash":"d4457edb4d30d91ba446fbeb811200dd180e15c5","modified":1597428881885},{"_id":"source/photography/index.md","hash":"e0d6bb616f057988af738a37e2bae755082f75fb","modified":1597428881885},{"_id":"themes/mytheme/layout/index.ejs","hash":"a34122a217dd9887a8c5071fe6a59582707e552b","modified":1597428881925},{"_id":"themes/mytheme/layout/archive.ejs","hash":"e7eca64dee6dea3d624e34f031a9be3bf3c87772","modified":1597428881925},{"_id":"themes/mytheme/layout/categories.ejs","hash":"4ac7ad4d319e5ee9c68e59e80a6f53a3b2de1467","modified":1597428881925},{"_id":"source/videography/index.md","hash":"6fc0d5c67a8f91ceea4279a8ec0077d89f32b7ed","modified":1597428881903},{"_id":"themes/mytheme/layout/layout.ejs","hash":"436adfb57e84222c0bc23038117d68dba061b58f","modified":1597428881925},{"_id":"themes/mytheme/layout/post.ejs","hash":"38382e9bbeb6b8d2eafbd53fff2984111f524c1a","modified":1597428881925},{"_id":"themes/mytheme/layout/page.ejs","hash":"53fbdf636f8ee043af7abb90930fa5459e49b85f","modified":1597428881925},{"_id":"themes/mytheme/source/CNAME","hash":"a2d4147afb7fb1c1b5223deeaf5d72d4eb82f0d6","modified":1597428881926},{"_id":"source/_posts/joker/joker-1-smile.jpg","hash":"8370650ce38968a11af24a03e29532d3b8046a00","modified":1597428881828},{"_id":"source/_posts/logistic-regression/SigmoidFunction.png","hash":"a4d6418d3d330a8c2602b768f5b70a48ca2d44de","modified":1597428881878},{"_id":"source/_posts/logistic-regression/SigmoidDerivative.png","hash":"937d609f776e165e45a517083db2bbce5f8321d1","modified":1597428881878},{"_id":"source/_posts/random-forest/RandomForestRegressor.png","hash":"7ff9fbc928872f0562a4d0dba7489d63c5218b29","modified":1597428881884},{"_id":"themes/mytheme/layout/_partial/after-footer.ejs","hash":"add37235560a0f3735459d4280c3cdaa9f9ac100","modified":1597428881924},{"_id":"themes/mytheme/layout/_partial/article-index.ejs","hash":"0545586badab8bbb0727d57db51fc19bdb8dcbf3","modified":1597428881924},{"_id":"themes/mytheme/layout/_partial/article-full.ejs","hash":"0fcd48e87637d0ec6e6aeb2ded875579c4553c3b","modified":1597428881924},{"_id":"themes/mytheme/layout/_partial/categories.ejs","hash":"62796c118a44fad17b18bb5d51361f71275e3a5c","modified":1597428881924},{"_id":"themes/mytheme/layout/_partial/footer.ejs","hash":"15556afa3ccd8daec6d4cd173bac8a6bbb54a148","modified":1597428881924},{"_id":"themes/mytheme/layout/_partial/head.ejs","hash":"bfdf20806d48ba491d0d7c00c539771d22b89250","modified":1597428881924},{"_id":"themes/mytheme/layout/_partial/header.ejs","hash":"12913c298b92191807b93ce3ea0a31b165028af9","modified":1597428881924},{"_id":"themes/mytheme/layout/_partial/page-full.ejs","hash":"63c18acb0430e55cbb403590508b17978e14398d","modified":1597428881925},{"_id":"themes/mytheme/source/css/styles.css","hash":"ad48554a9f7733891d0678d9512c504833c22cfc","modified":1597428881926},{"_id":"source/_posts/decision-tree/DecisionTreeClassifier.png","hash":"10a3542e9ad4189174e1a18fb6cbcf679051926a","modified":1597428881826},{"_id":"themes/mytheme/source/images/wechatID.JPG","hash":"7dbccebead009fb25fbc80481e37163d5ae868f7","modified":1597428881927},{"_id":"source/_posts/decision-tree/DecisionTreeRegressor.png","hash":"1165dc1607562e7743c4f5826194552ff83739f6","modified":1597428881828},{"_id":"source/_posts/neural-network/ArtificialNeuralNetwork.png","hash":"7446181073ed9b24a004d3086d55aba0346cb71f","modified":1597428881879},{"_id":"source/photography/photos/snow.jpg","hash":"008118f12a38df305f05b37e9f4579b3384759f3","modified":1597428881900},{"_id":"source/photography/photos/pisatower.jpg","hash":"d7defa1fc0a9904833227d4c3dbf7722c6c1f33c","modified":1597428881899},{"_id":"source/photography/photos/eiffeltower.jpg","hash":"cec2592a1ae1292241ffbf247582c681e1ee1f6e","modified":1597428881893},{"_id":"source/photography/photos/wall.jpg","hash":"72efe62999b543d48017a0617f269f5a29b5938d","modified":1597428881903},{"_id":"source/_posts/joker/joker-4-stairs.jpg","hash":"a140a036bbcb8f33e66b5685a03919915dc31043","modified":1597428881843},{"_id":"source/photography/photos/jw.jpg","hash":"1ab7245a3a4b48f55a8f0f1351e6dfd3b49891c6","modified":1597428881896},{"_id":"source/_posts/joker/joker-2-bus.jpg","hash":"50a093973550de15f37ba304a2b63b39891ef491","modified":1597428881829},{"_id":"source/_posts/notre-dame-cathedral/NotreDameCathedral.jpg","hash":"a2fc5107e6e3efefcde2830ca9d17584e2646156","modified":1597428881883},{"_id":"source/photography/photos/bw.jpg","hash":"341b1a6d0ecd78c079e464ccd338ed64e0b349b5","modified":1597428881891},{"_id":"source/_posts/joker/joker-3-card.png","hash":"b5ca18938e93af6eb91be7d2f2ad7c4b71d4c5c5","modified":1597428881842},{"_id":"source/_posts/joker/joker-5-notebook.png","hash":"d25661e272633c4af634e0cb79ba9dcea8a7643d","modified":1597428881857},{"_id":"source/_posts/joker/joker-7-gun.png","hash":"da4313d2c1cc7e5348b34ac2deb2c5d3aa6cf4ce","modified":1597428881870},{"_id":"source/_posts/joker/joker-6-dont-smile.png","hash":"7b156db2b756f58e719c5a5d9cc8a7b709fabcf1","modified":1597428881864},{"_id":"source/_posts/joker/joker-8-blood-smile.png","hash":"216e3a1db432b855c14da28c0be07b7a648e8a91","modified":1597428881877},{"_id":"public/about/index.html","hash":"c773a55c581e6757feeb3967612cc5fef192909c","modified":1597434479944},{"_id":"public/categories/index.html","hash":"49c7239cb53b83707342a1c828fbd2379229a8e7","modified":1597434479945},{"_id":"public/photography/index.html","hash":"d60d05ea9f4a66a6e2dcc07c555173c5771d9bdf","modified":1597434479953},{"_id":"public/videography/index.html","hash":"d81b18abaa3941788c7075dad914f2f160f98f32","modified":1597434479954},{"_id":"public/Life/joker/index.html","hash":"46db5fbaa1698e296e2a13930637402ba9972f76","modified":1597434479954},{"_id":"public/Machine-Learning/backpropagation/index.html","hash":"ca7bf9bb23c923bfb1d6766144d0d2f0a3301b3a","modified":1597434479955},{"_id":"public/Machine-Learning/logistic-regression/index.html","hash":"ca4e91cc26112f581afb7d689b6b030611b55854","modified":1597434479955},{"_id":"public/Life/notre-dame-cathedral/index.html","hash":"5d5092d7e30dc2f8e4aab644e8bf715c73128364","modified":1597434479955},{"_id":"public/Machine-Learning/linear-regression/index.html","hash":"638abe463d5d212c63e7f1cb0101d6d8f6087bb8","modified":1597434479955},{"_id":"public/Machine-Learning/neural-network/index.html","hash":"334d9f9caf140a3b0c163e5e5767ee32de653be1","modified":1597434479955},{"_id":"public/Machine-Learning/random-forest/index.html","hash":"57b5c40bbe301a4d703249cf0149d74a182bc3bb","modified":1597434479955},{"_id":"public/Machine-Learning/decision-tree/index.html","hash":"ccc8036305084a469d88c6d8b48f4097ddec40c8","modified":1597434479955},{"_id":"public/Life/old-melody/index.html","hash":"0f1307ecd3df37cae94c3877623224bf82e453a9","modified":1597434479955},{"_id":"public/Uncategorized/untitle/index.html","hash":"71078ac836e035f8249fdd793800650e10b2e058","modified":1597434479955},{"_id":"public/Machine-Learning/begin-machine-learning/index.html","hash":"0654a2a685c8e5a30ad9e702f03eefc40ea32c62","modified":1597434479955},{"_id":"public/categories/Machine-Learning/index.html","hash":"9f3386aa339f8a372a71cfc90c2de141855b92ad","modified":1597434479955},{"_id":"public/categories/Life/index.html","hash":"baac40ba92e225e0f92c865346e4f7aac69be864","modified":1597434479955},{"_id":"public/categories/Uncategorized/index.html","hash":"a119b10e8072cc1a70f254221868884033478284","modified":1597434479955},{"_id":"public/index.html","hash":"b839793867be68264b3d66ba359e4a9d6cf098bd","modified":1597434479955},{"_id":"public/page/2/index.html","hash":"963e52c60121cca30d60d6598fcf56a1778e7761","modified":1597434479955},{"_id":"public/archives/index.html","hash":"e73f945e2636abbe4945fc64a09ee1a8ea345c4b","modified":1597434479955},{"_id":"public/archives/page/2/index.html","hash":"83122ca5e9015736fda83ef0230d8acccd059de7","modified":1597434479955},{"_id":"public/archives/2019/index.html","hash":"7ac183226acd6be9222f90c695eb547e9864f6df","modified":1597434479956},{"_id":"public/archives/2019/page/2/index.html","hash":"1cee0d27ae5bf31ce2a65ea71a0dda4ba80b2925","modified":1597434479956},{"_id":"public/archives/2019/03/index.html","hash":"987ac4fecc765ec4d20da891ec2f5839d7bab6fd","modified":1597434479956},{"_id":"public/archives/2019/04/index.html","hash":"5a117e1bad0b2de838ff630f2138253039c8e65d","modified":1597434479956},{"_id":"public/archives/2019/05/index.html","hash":"ed501e9ad9825be0fb7377d416f72203c07626b1","modified":1597434479956},{"_id":"public/archives/2019/11/index.html","hash":"e52ce33650b92aea0fbce6da6fa5b6c686ad50f2","modified":1597434479956},{"_id":"public/Life/ten-years/index.html","hash":"59b12406758962e448d954a19c5d6ea0cc9fe22f","modified":1597440405957},{"_id":"public/archives/2020/index.html","hash":"dbf26cffee01b7f3343c63a76380c487ab4b371b","modified":1597434479957},{"_id":"public/archives/2020/08/index.html","hash":"85c4bd27d68794fb9030e98af60495c36c940189","modified":1597434479957}],"Category":[{"name":"Machine Learning","_id":"ckduknz550004sniy0pkj6m73"},{"name":"Life","_id":"ckduknz5f000gsniy3szcyoqu"},{"name":"Uncategorized","_id":"ckduknz5k000tsniyj2se9wow"}],"Data":[],"Page":[{"title":"About","date":"2019-03-26T05:31:58.000Z","_content":"<center> <h4> \nLife is Short, Do stuff that Matters.\n\n<hr>\n\n人生短暂，为自己而活！\n<h4></center>","source":"about/index.md","raw":"---\ntitle: About\ndate: 2019-03-25 22:31:58\n---\n<center> <h4> \nLife is Short, Do stuff that Matters.\n\n<hr>\n\n人生短暂，为自己而活！\n<h4></center>","updated":"2020-08-14T18:14:41.885Z","path":"about/index.html","comments":1,"layout":"page","_id":"ckduknz520001sniy7gmb48tm","content":"<center> <h4><br>Life is Short, Do stuff that Matters.<br><br><hr><br><br>人生短暂，为自己而活！<br><h4></h4></h4></center>","site":{"data":{}},"excerpt":"","more":"<center> <h4><br>Life is Short, Do stuff that Matters.<br><br><hr><br><br>人生短暂，为自己而活！<br><h4></h4></h4></center>"},{"title":"Categories","layout":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: Categories\nlayout: categories\n---\n","date":"2020-08-14T18:14:41.885Z","updated":"2020-08-14T18:14:41.885Z","path":"categories/index.html","comments":1,"_id":"ckduknz540003sniypgfaq1yc","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"Photography","date":"2019-03-30T19:55:15.000Z","_content":"<img src=\"./photos/jw.jpg\"  width=\"100%\">\n\n<img src=\"./photos/eiffeltower.jpg\"  width=\"49%\"><img src=\"./photos/pisatower.jpg\" width=\"49%\" align=\"right\">\n\n<img src=\"./photos/bw.jpg\"  width=\"100%\">\n\n<img src=\"./photos/wall.jpg\"  width=\"100%\">\n\n<img src=\"./photos/snow.jpg\"  width=\"100%\">\n","source":"photography/index.md","raw":"---\ntitle: Photography\ndate: 2019-03-30 12:55:15\n---\n<img src=\"./photos/jw.jpg\"  width=\"100%\">\n\n<img src=\"./photos/eiffeltower.jpg\"  width=\"49%\"><img src=\"./photos/pisatower.jpg\" width=\"49%\" align=\"right\">\n\n<img src=\"./photos/bw.jpg\"  width=\"100%\">\n\n<img src=\"./photos/wall.jpg\"  width=\"100%\">\n\n<img src=\"./photos/snow.jpg\"  width=\"100%\">\n","updated":"2020-08-14T18:14:41.885Z","path":"photography/index.html","comments":1,"layout":"page","_id":"ckduknz5o000xsniyihnpwx7i","content":"<p><img src=\"./photos/jw.jpg\" width=\"100%\"></p>\n<p><img src=\"./photos/eiffeltower.jpg\" width=\"49%\"><img src=\"./photos/pisatower.jpg\" width=\"49%\" align=\"right\"></p>\n<p><img src=\"./photos/bw.jpg\" width=\"100%\"></p>\n<p><img src=\"./photos/wall.jpg\" width=\"100%\"></p>\n<p><img src=\"./photos/snow.jpg\" width=\"100%\"></p>\n","site":{"data":{}},"excerpt":"","more":"<p><img src=\"./photos/jw.jpg\" width=\"100%\"></p>\n<p><img src=\"./photos/eiffeltower.jpg\" width=\"49%\"><img src=\"./photos/pisatower.jpg\" width=\"49%\" align=\"right\"></p>\n<p><img src=\"./photos/bw.jpg\" width=\"100%\"></p>\n<p><img src=\"./photos/wall.jpg\" width=\"100%\"></p>\n<p><img src=\"./photos/snow.jpg\" width=\"100%\"></p>\n"},{"title":"Videography","date":"2019-04-02T05:42:35.000Z","_content":"<center> <h3>Trip to Alaska</h3> </center>\n<div class=\"video-container\"><iframe src=\"//www.youtube.com/embed/aU4pRWFJoWs\" frameborder=\"0\" allowfullscreen class=\"video\"></iframe></div>\n<hr><br>\n<center> <h3>Trip to Europe</h3> </center>\n<div class=\"video-container\"><iframe src=\"//www.youtube.com/embed/cDzGgm1SspY\" frameborder=\"0\" allowfullscreen class=\"video\"></iframe></div>\n<hr><br>\n<center> <h3>Trip to Chongqing & Chengdu</h3> </center>\n<div class=\"video-container\"><iframe src=\"//www.youtube.com/embed/8ctV5vTmFE8\" frameborder=\"0\" allowfullscreen class=\"video\"></iframe></div>\n<hr><br>\n<center> <h3>Trip to Glenwood Springs</h3> </center>\n<div class=\"video-container\"><iframe src=\"//www.youtube.com/embed/vM3QkD2kO5Y\" frameborder=\"0\" allowfullscreen class=\"video\"></iframe></div>\n<hr><br>\n<center> <h3>Trip to Seattle & Vancouver</h3> </center>\n<div class=\"video-container\"><iframe src=\"//www.youtube.com/embed/pY0F7yqVGNo\" frameborder=\"0\" allowfullscreen class=\"video\"></iframe></div>\n<hr><br>\n<center> <h3>Ski</h3> </center>\n<div class=\"video-container\"><iframe src=\"//www.youtube.com/embed/mf5vsytF380\" frameborder=\"0\" allowfullscreen class=\"video\"></iframe></div>\n<hr>","source":"videography/index.md","raw":"---\ntitle: Videography\ndate: 2019-04-01 22:42:35\n---\n<center> <h3>Trip to Alaska</h3> </center>\n<div class=\"video-container\"><iframe src=\"//www.youtube.com/embed/aU4pRWFJoWs\" frameborder=\"0\" allowfullscreen class=\"video\"></iframe></div>\n<hr><br>\n<center> <h3>Trip to Europe</h3> </center>\n<div class=\"video-container\"><iframe src=\"//www.youtube.com/embed/cDzGgm1SspY\" frameborder=\"0\" allowfullscreen class=\"video\"></iframe></div>\n<hr><br>\n<center> <h3>Trip to Chongqing & Chengdu</h3> </center>\n<div class=\"video-container\"><iframe src=\"//www.youtube.com/embed/8ctV5vTmFE8\" frameborder=\"0\" allowfullscreen class=\"video\"></iframe></div>\n<hr><br>\n<center> <h3>Trip to Glenwood Springs</h3> </center>\n<div class=\"video-container\"><iframe src=\"//www.youtube.com/embed/vM3QkD2kO5Y\" frameborder=\"0\" allowfullscreen class=\"video\"></iframe></div>\n<hr><br>\n<center> <h3>Trip to Seattle & Vancouver</h3> </center>\n<div class=\"video-container\"><iframe src=\"//www.youtube.com/embed/pY0F7yqVGNo\" frameborder=\"0\" allowfullscreen class=\"video\"></iframe></div>\n<hr><br>\n<center> <h3>Ski</h3> </center>\n<div class=\"video-container\"><iframe src=\"//www.youtube.com/embed/mf5vsytF380\" frameborder=\"0\" allowfullscreen class=\"video\"></iframe></div>\n<hr>","updated":"2020-08-14T18:14:41.903Z","path":"videography/index.html","comments":1,"layout":"page","_id":"ckduknz5p000ysniya20yp8xv","content":"<p><center> <h3>Trip to Alaska</h3> </center></p>\n<p><div class=\"video-container\"><iframe src=\"//www.youtube.com/embed/aU4pRWFJoWs\" frameborder=\"0\" allowfullscreen class=\"video\"></iframe></div></p>\n<p><hr><br></p>\n<p><center> <h3>Trip to Europe</h3> </center></p>\n<p><div class=\"video-container\"><iframe src=\"//www.youtube.com/embed/cDzGgm1SspY\" frameborder=\"0\" allowfullscreen class=\"video\"></iframe></div></p>\n<p><hr><br></p>\n<p><center> <h3>Trip to Chongqing &amp; Chengdu</h3> </center></p>\n<p><div class=\"video-container\"><iframe src=\"//www.youtube.com/embed/8ctV5vTmFE8\" frameborder=\"0\" allowfullscreen class=\"video\"></iframe></div></p>\n<p><hr><br></p>\n<p><center> <h3>Trip to Glenwood Springs</h3> </center></p>\n<p><div class=\"video-container\"><iframe src=\"//www.youtube.com/embed/vM3QkD2kO5Y\" frameborder=\"0\" allowfullscreen class=\"video\"></iframe></div></p>\n<p><hr><br></p>\n<p><center> <h3>Trip to Seattle &amp; Vancouver</h3> </center></p>\n<p><div class=\"video-container\"><iframe src=\"//www.youtube.com/embed/pY0F7yqVGNo\" frameborder=\"0\" allowfullscreen class=\"video\"></iframe></div></p>\n<p><hr><br></p>\n<p><center> <h3>Ski</h3> </center></p>\n<p><div class=\"video-container\"><iframe src=\"//www.youtube.com/embed/mf5vsytF380\" frameborder=\"0\" allowfullscreen class=\"video\"></iframe></div></p>\n<hr>","site":{"data":{}},"excerpt":"","more":"<p><center> <h3>Trip to Alaska</h3> </center></p>\n<p><div class=\"video-container\"><iframe src=\"//www.youtube.com/embed/aU4pRWFJoWs\" frameborder=\"0\" allowfullscreen class=\"video\"></iframe></div></p>\n<p><hr><br></p>\n<p><center> <h3>Trip to Europe</h3> </center></p>\n<p><div class=\"video-container\"><iframe src=\"//www.youtube.com/embed/cDzGgm1SspY\" frameborder=\"0\" allowfullscreen class=\"video\"></iframe></div></p>\n<p><hr><br></p>\n<p><center> <h3>Trip to Chongqing &amp; Chengdu</h3> </center></p>\n<p><div class=\"video-container\"><iframe src=\"//www.youtube.com/embed/8ctV5vTmFE8\" frameborder=\"0\" allowfullscreen class=\"video\"></iframe></div></p>\n<p><hr><br></p>\n<p><center> <h3>Trip to Glenwood Springs</h3> </center></p>\n<p><div class=\"video-container\"><iframe src=\"//www.youtube.com/embed/vM3QkD2kO5Y\" frameborder=\"0\" allowfullscreen class=\"video\"></iframe></div></p>\n<p><hr><br></p>\n<p><center> <h3>Trip to Seattle &amp; Vancouver</h3> </center></p>\n<p><div class=\"video-container\"><iframe src=\"//www.youtube.com/embed/pY0F7yqVGNo\" frameborder=\"0\" allowfullscreen class=\"video\"></iframe></div></p>\n<p><hr><br></p>\n<p><center> <h3>Ski</h3> </center></p>\n<p><div class=\"video-container\"><iframe src=\"//www.youtube.com/embed/mf5vsytF380\" frameborder=\"0\" allowfullscreen class=\"video\"></iframe></div></p>\n<hr>"}],"Post":[{"title":"Backpropagation","date":"2019-05-12T01:04:41.000Z","_content":"\n#### Loss Function\nWe use binary log loss (**cross entropy**). \n$$ Loss = \\frac{1}{N} \\sum_{i=1}^N -{(y_i\\log(p_i) + (1 - y_i)\\log(1 - p_i))} $$\nRemember: Here the **log** is natual-log ($ln$), because the exponential $e$ should match $ln$.\n<hr>\n\n#### Forward\nFrom the **Neural Network**, We using forward function to find the predict value and **Loss** value.\n\nPredict Value: The value we using the input and weight to calculate the predict for output.\n\nLoss Value: We using the **Predict Value** and the **Actual Value** into Loss Function to get the Loss Value for current prediction.\n<hr>\n\n#### Backward\nWe using the **Neural Network**, start from the **Loss** value backward, and use Derivative for each **Gate** go back to change the value on each weight.\n\n$$ \\frac{\\partial y}{\\partial x} = \\frac{\\partial y}{\\partial y} \\cdot \\frac{\\partial y}{\\partial f} \\cdot \\frac{\\partial f}{\\partial g} \\cdot \\frac{\\partial g}{\\partial x}$$ \n<hr>\n\n#### Code Example\nThe input value is **Variable**, the weight value and learning rate we put into **Parameter**.\n\n{% codeblock lang:python %}\n# y = a * x1 + b\n# y = (a * x1 + b - y1)^2\n# learning_rate = 0.1, a=0, b=0\n\nx1 = graph.Variable(0.5)\ny1 = graph.Variable(1.7)\na = graph.Parameter(0, 0.1)\nb = graph.Parameter(0, 0.1)\nprint(a.value, b.value)\n\nloss = graph.Add(\n        graph.Add(graph.Mul(x1, a), b),\n        graph.Mul(y1, graph.Variable(-1)))\n\nloss.forward() # -> your curretn loss\nloss.backward(1) # -> cal backward to change weight, inital the '1' for input dy/dy\n{% endcodeblock %}","source":"_posts/backpropagation.md","raw":"---\ntitle: Backpropagation\ncategories: Machine Learning\ndate: 2019-05-11 18:04:41\n---\n\n#### Loss Function\nWe use binary log loss (**cross entropy**). \n$$ Loss = \\frac{1}{N} \\sum_{i=1}^N -{(y_i\\log(p_i) + (1 - y_i)\\log(1 - p_i))} $$\nRemember: Here the **log** is natual-log ($ln$), because the exponential $e$ should match $ln$.\n<hr>\n\n#### Forward\nFrom the **Neural Network**, We using forward function to find the predict value and **Loss** value.\n\nPredict Value: The value we using the input and weight to calculate the predict for output.\n\nLoss Value: We using the **Predict Value** and the **Actual Value** into Loss Function to get the Loss Value for current prediction.\n<hr>\n\n#### Backward\nWe using the **Neural Network**, start from the **Loss** value backward, and use Derivative for each **Gate** go back to change the value on each weight.\n\n$$ \\frac{\\partial y}{\\partial x} = \\frac{\\partial y}{\\partial y} \\cdot \\frac{\\partial y}{\\partial f} \\cdot \\frac{\\partial f}{\\partial g} \\cdot \\frac{\\partial g}{\\partial x}$$ \n<hr>\n\n#### Code Example\nThe input value is **Variable**, the weight value and learning rate we put into **Parameter**.\n\n{% codeblock lang:python %}\n# y = a * x1 + b\n# y = (a * x1 + b - y1)^2\n# learning_rate = 0.1, a=0, b=0\n\nx1 = graph.Variable(0.5)\ny1 = graph.Variable(1.7)\na = graph.Parameter(0, 0.1)\nb = graph.Parameter(0, 0.1)\nprint(a.value, b.value)\n\nloss = graph.Add(\n        graph.Add(graph.Mul(x1, a), b),\n        graph.Mul(y1, graph.Variable(-1)))\n\nloss.forward() # -> your curretn loss\nloss.backward(1) # -> cal backward to change weight, inital the '1' for input dy/dy\n{% endcodeblock %}","slug":"backpropagation","published":1,"updated":"2020-08-14T18:14:41.825Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckduknz4y0000sniyby6dqxs8","content":"<h4 id=\"Loss-Function\"><a href=\"#Loss-Function\" class=\"headerlink\" title=\"Loss Function\"></a>Loss Function</h4><p>We use binary log loss (<strong>cross entropy</strong>).<br>$$ Loss = \\frac{1}{N} \\sum_{i=1}^N -{(y_i\\log(p_i) + (1 - y_i)\\log(1 - p_i))} $$<br>Remember: Here the <strong>log</strong> is natual-log ($ln$), because the exponential $e$ should match $ln$.</p>\n<hr>\n\n<h4 id=\"Forward\"><a href=\"#Forward\" class=\"headerlink\" title=\"Forward\"></a>Forward</h4><p>From the <strong>Neural Network</strong>, We using forward function to find the predict value and <strong>Loss</strong> value.</p>\n<p>Predict Value: The value we using the input and weight to calculate the predict for output.</p>\n<p>Loss Value: We using the <strong>Predict Value</strong> and the <strong>Actual Value</strong> into Loss Function to get the Loss Value for current prediction.</p>\n<hr>\n\n<h4 id=\"Backward\"><a href=\"#Backward\" class=\"headerlink\" title=\"Backward\"></a>Backward</h4><p>We using the <strong>Neural Network</strong>, start from the <strong>Loss</strong> value backward, and use Derivative for each <strong>Gate</strong> go back to change the value on each weight.</p>\n<p>$$ \\frac{\\partial y}{\\partial x} = \\frac{\\partial y}{\\partial y} \\cdot \\frac{\\partial y}{\\partial f} \\cdot \\frac{\\partial f}{\\partial g} \\cdot \\frac{\\partial g}{\\partial x}$$ </p>\n<hr>\n\n<h4 id=\"Code-Example\"><a href=\"#Code-Example\" class=\"headerlink\" title=\"Code Example\"></a>Code Example</h4><p>The input value is <strong>Variable</strong>, the weight value and learning rate we put into <strong>Parameter</strong>.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># y = a * x1 + b</span></span><br><span class=\"line\"><span class=\"comment\"># y = (a * x1 + b - y1)^2</span></span><br><span class=\"line\"><span class=\"comment\"># learning_rate = 0.1, a=0, b=0</span></span><br><span class=\"line\"></span><br><span class=\"line\">x1 = graph.Variable(<span class=\"number\">0.5</span>)</span><br><span class=\"line\">y1 = graph.Variable(<span class=\"number\">1.7</span>)</span><br><span class=\"line\">a = graph.Parameter(<span class=\"number\">0</span>, <span class=\"number\">0.1</span>)</span><br><span class=\"line\">b = graph.Parameter(<span class=\"number\">0</span>, <span class=\"number\">0.1</span>)</span><br><span class=\"line\">print(a.value, b.value)</span><br><span class=\"line\"></span><br><span class=\"line\">loss = graph.Add(</span><br><span class=\"line\">        graph.Add(graph.Mul(x1, a), b),</span><br><span class=\"line\">        graph.Mul(y1, graph.Variable(<span class=\"number\">-1</span>)))</span><br><span class=\"line\"></span><br><span class=\"line\">loss.forward() <span class=\"comment\"># -&gt; your curretn loss</span></span><br><span class=\"line\">loss.backward(<span class=\"number\">1</span>) <span class=\"comment\"># -&gt; cal backward to change weight, inital the '1' for input dy/dy</span></span><br></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"","more":"<h4 id=\"Loss-Function\"><a href=\"#Loss-Function\" class=\"headerlink\" title=\"Loss Function\"></a>Loss Function</h4><p>We use binary log loss (<strong>cross entropy</strong>).<br>$$ Loss = \\frac{1}{N} \\sum_{i=1}^N -{(y_i\\log(p_i) + (1 - y_i)\\log(1 - p_i))} $$<br>Remember: Here the <strong>log</strong> is natual-log ($ln$), because the exponential $e$ should match $ln$.</p>\n<hr>\n\n<h4 id=\"Forward\"><a href=\"#Forward\" class=\"headerlink\" title=\"Forward\"></a>Forward</h4><p>From the <strong>Neural Network</strong>, We using forward function to find the predict value and <strong>Loss</strong> value.</p>\n<p>Predict Value: The value we using the input and weight to calculate the predict for output.</p>\n<p>Loss Value: We using the <strong>Predict Value</strong> and the <strong>Actual Value</strong> into Loss Function to get the Loss Value for current prediction.</p>\n<hr>\n\n<h4 id=\"Backward\"><a href=\"#Backward\" class=\"headerlink\" title=\"Backward\"></a>Backward</h4><p>We using the <strong>Neural Network</strong>, start from the <strong>Loss</strong> value backward, and use Derivative for each <strong>Gate</strong> go back to change the value on each weight.</p>\n<p>$$ \\frac{\\partial y}{\\partial x} = \\frac{\\partial y}{\\partial y} \\cdot \\frac{\\partial y}{\\partial f} \\cdot \\frac{\\partial f}{\\partial g} \\cdot \\frac{\\partial g}{\\partial x}$$ </p>\n<hr>\n\n<h4 id=\"Code-Example\"><a href=\"#Code-Example\" class=\"headerlink\" title=\"Code Example\"></a>Code Example</h4><p>The input value is <strong>Variable</strong>, the weight value and learning rate we put into <strong>Parameter</strong>.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># y = a * x1 + b</span></span><br><span class=\"line\"><span class=\"comment\"># y = (a * x1 + b - y1)^2</span></span><br><span class=\"line\"><span class=\"comment\"># learning_rate = 0.1, a=0, b=0</span></span><br><span class=\"line\"></span><br><span class=\"line\">x1 = graph.Variable(<span class=\"number\">0.5</span>)</span><br><span class=\"line\">y1 = graph.Variable(<span class=\"number\">1.7</span>)</span><br><span class=\"line\">a = graph.Parameter(<span class=\"number\">0</span>, <span class=\"number\">0.1</span>)</span><br><span class=\"line\">b = graph.Parameter(<span class=\"number\">0</span>, <span class=\"number\">0.1</span>)</span><br><span class=\"line\">print(a.value, b.value)</span><br><span class=\"line\"></span><br><span class=\"line\">loss = graph.Add(</span><br><span class=\"line\">        graph.Add(graph.Mul(x1, a), b),</span><br><span class=\"line\">        graph.Mul(y1, graph.Variable(<span class=\"number\">-1</span>)))</span><br><span class=\"line\"></span><br><span class=\"line\">loss.forward() <span class=\"comment\"># -&gt; your curretn loss</span></span><br><span class=\"line\">loss.backward(<span class=\"number\">1</span>) <span class=\"comment\"># -&gt; cal backward to change weight, inital the '1' for input dy/dy</span></span><br></pre></td></tr></table></figure>"},{"title":"Decision Tree","date":"2019-04-06T21:15:50.000Z","_content":"\n### Decision Tree Regression\n\nUsing features data from DataFrame to train a regression tree model. The way to find the best split value is to find the best **MSE** in each feature type. When we found the **BEST MSE** value, the model will set it as the split value for current level. Separate the X and y into left subtree and right subtree, and keep going to train the model tree from subtree. \n\nMore Info Using Scikit-Learn Built-in library: [Scikit-Learn Decision Tree Regression](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)\n\n- max_depth: int or None, optional (default=None)\nThe maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n\n- min_samples_leaf: int, float, optional (default=1)\nThe minimum number of samples required to be at a leaf node.\n\n#### Methods\n- fit(self, X: pd.DataFrame, y: np.array)\t\nBuild a decision tree regressor from the training set (X, y).\n\n- predict(self, X: pd.DataFrame)    \nPredict regression value for X.\n\n- score(self, X: pd.DataFrame, y: np.array)\nReturns the coefficient of determination $R^2$ of the prediction.\n\n#### Graphic Example \n<img src=\"{% asset_path DecisionTreeRegressor.png %}\"  width=\"100%\">\n\n<br><hr>\n\n### Decision Tree Classification\nUsing features data from DataFrame to train a regression tree model. The way to find the best split value is to find the best average subtree of **GINI Impurity X Data Weight** in each feature type. When we found the **BEST (GINI Impurity X Data Weight)** value, the model will set it as the split value for current level. Separate the X and y into left subtree and right subtree, and keep going to train the model tree from subtree.\n\nMore Info Using Scikit-Learn Built-in library. [Scikit-Learn Decision Tree Regression](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) \n\n- GINI Impurity:\n$$ GINI =1-\\sum_{i=1}^{N}{p_{i}}^{2} $$\n\n{% codeblock lang:python %}\ndef calc_gini(y, valueType):\n    subvalue = [0] * len(valueType)\n\n    for i in y:\n        if i == valueType[i]:\n            subvalue[i] += 1\n\n    giniSum = 0\n    for i in subvalue:\n        giniSum += (i/sum(subvalue)) ** 2\n    subgini = 1-giniSum\n\n    return subgini\n{% endcodeblock %}\n\n- Data Weight: \n$$ Data Weight = {\\sum_{i=1}^{N}{SubtreeNode}\\over{TotalNode}} $$\n\n{% codeblock lang:python %}\nif left.any() and right.any():\n\tcur_giniWeight = self.calc_gini(y[left], self.valueType) * (len(y[left]) / len(y)) \\\n\t+ self.calc_gini(y[right], self.valueType) * (len(y[right])/len(y))\n{% endcodeblock %}\n\n- max_depth: int or None, optional (default=None)\nThe maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n\n- min_samples_leaf: int, float, optional (default=1)\nThe minimum number of samples required to be at a leaf node.\n\n#### Methods\n- fit(self, X: pd.DataFrame, y: np.array)\t\nBuild a decision tree classifier from the training set (X, y).\n\n- predict(self, X: pd.DataFrame)    \nPredict class value for X.\n\n- score(self, X: pd.DataFrame, y: np.array)\nReturns the mean accuracy on the given test data and labels.\n\n#### Graphic Example \n<img src=\"{% asset_path DecisionTreeClassifier.png %}\"  width=\"100%\">","source":"_posts/decision-tree.md","raw":"---\ntitle: Decision Tree\ncategories: Machine Learning\ndate: 2019-04-06 14:15:50\n---\n\n### Decision Tree Regression\n\nUsing features data from DataFrame to train a regression tree model. The way to find the best split value is to find the best **MSE** in each feature type. When we found the **BEST MSE** value, the model will set it as the split value for current level. Separate the X and y into left subtree and right subtree, and keep going to train the model tree from subtree. \n\nMore Info Using Scikit-Learn Built-in library: [Scikit-Learn Decision Tree Regression](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)\n\n- max_depth: int or None, optional (default=None)\nThe maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n\n- min_samples_leaf: int, float, optional (default=1)\nThe minimum number of samples required to be at a leaf node.\n\n#### Methods\n- fit(self, X: pd.DataFrame, y: np.array)\t\nBuild a decision tree regressor from the training set (X, y).\n\n- predict(self, X: pd.DataFrame)    \nPredict regression value for X.\n\n- score(self, X: pd.DataFrame, y: np.array)\nReturns the coefficient of determination $R^2$ of the prediction.\n\n#### Graphic Example \n<img src=\"{% asset_path DecisionTreeRegressor.png %}\"  width=\"100%\">\n\n<br><hr>\n\n### Decision Tree Classification\nUsing features data from DataFrame to train a regression tree model. The way to find the best split value is to find the best average subtree of **GINI Impurity X Data Weight** in each feature type. When we found the **BEST (GINI Impurity X Data Weight)** value, the model will set it as the split value for current level. Separate the X and y into left subtree and right subtree, and keep going to train the model tree from subtree.\n\nMore Info Using Scikit-Learn Built-in library. [Scikit-Learn Decision Tree Regression](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) \n\n- GINI Impurity:\n$$ GINI =1-\\sum_{i=1}^{N}{p_{i}}^{2} $$\n\n{% codeblock lang:python %}\ndef calc_gini(y, valueType):\n    subvalue = [0] * len(valueType)\n\n    for i in y:\n        if i == valueType[i]:\n            subvalue[i] += 1\n\n    giniSum = 0\n    for i in subvalue:\n        giniSum += (i/sum(subvalue)) ** 2\n    subgini = 1-giniSum\n\n    return subgini\n{% endcodeblock %}\n\n- Data Weight: \n$$ Data Weight = {\\sum_{i=1}^{N}{SubtreeNode}\\over{TotalNode}} $$\n\n{% codeblock lang:python %}\nif left.any() and right.any():\n\tcur_giniWeight = self.calc_gini(y[left], self.valueType) * (len(y[left]) / len(y)) \\\n\t+ self.calc_gini(y[right], self.valueType) * (len(y[right])/len(y))\n{% endcodeblock %}\n\n- max_depth: int or None, optional (default=None)\nThe maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n\n- min_samples_leaf: int, float, optional (default=1)\nThe minimum number of samples required to be at a leaf node.\n\n#### Methods\n- fit(self, X: pd.DataFrame, y: np.array)\t\nBuild a decision tree classifier from the training set (X, y).\n\n- predict(self, X: pd.DataFrame)    \nPredict class value for X.\n\n- score(self, X: pd.DataFrame, y: np.array)\nReturns the mean accuracy on the given test data and labels.\n\n#### Graphic Example \n<img src=\"{% asset_path DecisionTreeClassifier.png %}\"  width=\"100%\">","slug":"decision-tree","published":1,"updated":"2020-08-14T18:14:41.825Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckduknz530002sniy3ed3vsdp","content":"<h3 id=\"Decision-Tree-Regression\"><a href=\"#Decision-Tree-Regression\" class=\"headerlink\" title=\"Decision Tree Regression\"></a>Decision Tree Regression</h3><p>Using features data from DataFrame to train a regression tree model. The way to find the best split value is to find the best <strong>MSE</strong> in each feature type. When we found the <strong>BEST MSE</strong> value, the model will set it as the split value for current level. Separate the X and y into left subtree and right subtree, and keep going to train the model tree from subtree. </p>\n<p>More Info Using Scikit-Learn Built-in library: <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\" target=\"_blank\" rel=\"noopener\">Scikit-Learn Decision Tree Regression</a></p>\n<ul>\n<li><p>max_depth: int or None, optional (default=None)<br>The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.</p>\n</li>\n<li><p>min_samples_leaf: int, float, optional (default=1)<br>The minimum number of samples required to be at a leaf node.</p>\n</li>\n</ul>\n<h4 id=\"Methods\"><a href=\"#Methods\" class=\"headerlink\" title=\"Methods\"></a>Methods</h4><ul>\n<li><p>fit(self, X: pd.DataFrame, y: np.array)<br>Build a decision tree regressor from the training set (X, y).</p>\n</li>\n<li><p>predict(self, X: pd.DataFrame)<br>Predict regression value for X.</p>\n</li>\n<li><p>score(self, X: pd.DataFrame, y: np.array)<br>Returns the coefficient of determination $R^2$ of the prediction.</p>\n</li>\n</ul>\n<h4 id=\"Graphic-Example\"><a href=\"#Graphic-Example\" class=\"headerlink\" title=\"Graphic Example\"></a>Graphic Example</h4><p><img src=\"/Machine-Learning/decision-tree/DecisionTreeRegressor.png\" width=\"100%\"></p>\n<p><br><hr></p>\n<h3 id=\"Decision-Tree-Classification\"><a href=\"#Decision-Tree-Classification\" class=\"headerlink\" title=\"Decision Tree Classification\"></a>Decision Tree Classification</h3><p>Using features data from DataFrame to train a regression tree model. The way to find the best split value is to find the best average subtree of <strong>GINI Impurity X Data Weight</strong> in each feature type. When we found the <strong>BEST (GINI Impurity X Data Weight)</strong> value, the model will set it as the split value for current level. Separate the X and y into left subtree and right subtree, and keep going to train the model tree from subtree.</p>\n<p>More Info Using Scikit-Learn Built-in library. <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\" target=\"_blank\" rel=\"noopener\">Scikit-Learn Decision Tree Regression</a> </p>\n<ul>\n<li>GINI Impurity:<br>$$ GINI =1-\\sum_{i=1}^{N}{p_{i}}^{2} $$</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">calc_gini</span><span class=\"params\">(y, valueType)</span>:</span></span><br><span class=\"line\">    subvalue = [<span class=\"number\">0</span>] * len(valueType)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> y:</span><br><span class=\"line\">        <span class=\"keyword\">if</span> i == valueType[i]:</span><br><span class=\"line\">            subvalue[i] += <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">    giniSum = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> subvalue:</span><br><span class=\"line\">        giniSum += (i/sum(subvalue)) ** <span class=\"number\">2</span></span><br><span class=\"line\">    subgini = <span class=\"number\">1</span>-giniSum</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> subgini</span><br></pre></td></tr></table></figure>\n<ul>\n<li>Data Weight:<br>$$ Data Weight = {\\sum_{i=1}^{N}{SubtreeNode}\\over{TotalNode}} $$</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> left.any() <span class=\"keyword\">and</span> right.any():</span><br><span class=\"line\">\tcur_giniWeight = self.calc_gini(y[left], self.valueType) * (len(y[left]) / len(y)) \\</span><br><span class=\"line\">\t+ self.calc_gini(y[right], self.valueType) * (len(y[right])/len(y))</span><br></pre></td></tr></table></figure>\n<ul>\n<li><p>max_depth: int or None, optional (default=None)<br>The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.</p>\n</li>\n<li><p>min_samples_leaf: int, float, optional (default=1)<br>The minimum number of samples required to be at a leaf node.</p>\n</li>\n</ul>\n<h4 id=\"Methods-1\"><a href=\"#Methods-1\" class=\"headerlink\" title=\"Methods\"></a>Methods</h4><ul>\n<li><p>fit(self, X: pd.DataFrame, y: np.array)<br>Build a decision tree classifier from the training set (X, y).</p>\n</li>\n<li><p>predict(self, X: pd.DataFrame)<br>Predict class value for X.</p>\n</li>\n<li><p>score(self, X: pd.DataFrame, y: np.array)<br>Returns the mean accuracy on the given test data and labels.</p>\n</li>\n</ul>\n<h4 id=\"Graphic-Example-1\"><a href=\"#Graphic-Example-1\" class=\"headerlink\" title=\"Graphic Example\"></a>Graphic Example</h4><p><img src=\"/Machine-Learning/decision-tree/DecisionTreeClassifier.png\" width=\"100%\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"Decision-Tree-Regression\"><a href=\"#Decision-Tree-Regression\" class=\"headerlink\" title=\"Decision Tree Regression\"></a>Decision Tree Regression</h3><p>Using features data from DataFrame to train a regression tree model. The way to find the best split value is to find the best <strong>MSE</strong> in each feature type. When we found the <strong>BEST MSE</strong> value, the model will set it as the split value for current level. Separate the X and y into left subtree and right subtree, and keep going to train the model tree from subtree. </p>\n<p>More Info Using Scikit-Learn Built-in library: <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\" target=\"_blank\" rel=\"noopener\">Scikit-Learn Decision Tree Regression</a></p>\n<ul>\n<li><p>max_depth: int or None, optional (default=None)<br>The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.</p>\n</li>\n<li><p>min_samples_leaf: int, float, optional (default=1)<br>The minimum number of samples required to be at a leaf node.</p>\n</li>\n</ul>\n<h4 id=\"Methods\"><a href=\"#Methods\" class=\"headerlink\" title=\"Methods\"></a>Methods</h4><ul>\n<li><p>fit(self, X: pd.DataFrame, y: np.array)<br>Build a decision tree regressor from the training set (X, y).</p>\n</li>\n<li><p>predict(self, X: pd.DataFrame)<br>Predict regression value for X.</p>\n</li>\n<li><p>score(self, X: pd.DataFrame, y: np.array)<br>Returns the coefficient of determination $R^2$ of the prediction.</p>\n</li>\n</ul>\n<h4 id=\"Graphic-Example\"><a href=\"#Graphic-Example\" class=\"headerlink\" title=\"Graphic Example\"></a>Graphic Example</h4><p><img src=\"/Machine-Learning/decision-tree/DecisionTreeRegressor.png\" width=\"100%\"></p>\n<p><br><hr></p>\n<h3 id=\"Decision-Tree-Classification\"><a href=\"#Decision-Tree-Classification\" class=\"headerlink\" title=\"Decision Tree Classification\"></a>Decision Tree Classification</h3><p>Using features data from DataFrame to train a regression tree model. The way to find the best split value is to find the best average subtree of <strong>GINI Impurity X Data Weight</strong> in each feature type. When we found the <strong>BEST (GINI Impurity X Data Weight)</strong> value, the model will set it as the split value for current level. Separate the X and y into left subtree and right subtree, and keep going to train the model tree from subtree.</p>\n<p>More Info Using Scikit-Learn Built-in library. <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\" target=\"_blank\" rel=\"noopener\">Scikit-Learn Decision Tree Regression</a> </p>\n<ul>\n<li>GINI Impurity:<br>$$ GINI =1-\\sum_{i=1}^{N}{p_{i}}^{2} $$</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">calc_gini</span><span class=\"params\">(y, valueType)</span>:</span></span><br><span class=\"line\">    subvalue = [<span class=\"number\">0</span>] * len(valueType)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> y:</span><br><span class=\"line\">        <span class=\"keyword\">if</span> i == valueType[i]:</span><br><span class=\"line\">            subvalue[i] += <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">    giniSum = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> subvalue:</span><br><span class=\"line\">        giniSum += (i/sum(subvalue)) ** <span class=\"number\">2</span></span><br><span class=\"line\">    subgini = <span class=\"number\">1</span>-giniSum</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> subgini</span><br></pre></td></tr></table></figure>\n<ul>\n<li>Data Weight:<br>$$ Data Weight = {\\sum_{i=1}^{N}{SubtreeNode}\\over{TotalNode}} $$</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> left.any() <span class=\"keyword\">and</span> right.any():</span><br><span class=\"line\">\tcur_giniWeight = self.calc_gini(y[left], self.valueType) * (len(y[left]) / len(y)) \\</span><br><span class=\"line\">\t+ self.calc_gini(y[right], self.valueType) * (len(y[right])/len(y))</span><br></pre></td></tr></table></figure>\n<ul>\n<li><p>max_depth: int or None, optional (default=None)<br>The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.</p>\n</li>\n<li><p>min_samples_leaf: int, float, optional (default=1)<br>The minimum number of samples required to be at a leaf node.</p>\n</li>\n</ul>\n<h4 id=\"Methods-1\"><a href=\"#Methods-1\" class=\"headerlink\" title=\"Methods\"></a>Methods</h4><ul>\n<li><p>fit(self, X: pd.DataFrame, y: np.array)<br>Build a decision tree classifier from the training set (X, y).</p>\n</li>\n<li><p>predict(self, X: pd.DataFrame)<br>Predict class value for X.</p>\n</li>\n<li><p>score(self, X: pd.DataFrame, y: np.array)<br>Returns the mean accuracy on the given test data and labels.</p>\n</li>\n</ul>\n<h4 id=\"Graphic-Example-1\"><a href=\"#Graphic-Example-1\" class=\"headerlink\" title=\"Graphic Example\"></a>Graphic Example</h4><p><img src=\"/Machine-Learning/decision-tree/DecisionTreeClassifier.png\" width=\"100%\"></p>\n"},{"title":"Begin Machine Learning","date":"2019-03-31T05:42:55.000Z","_content":"Welcome to Machine Learning. Everything is imported from class lectures and backup for future study.\n<br>\n\n### Train_Test_Split:\n```\nInput: X, y, test_size, shuffle, random_state\nOutput: X_train, X_test, y_train, y_test\n```\n- (X, y): features and the target variable.\n- (test_size): between 0 and 1 - how much to allocate to the test set; the rest goes to the train set. \n- (shuffle): if True, shuffle the dataset, otherwise not.\n- (random_state): integer; if None, then results are random, otherwise fixed to a given seed. \n<br>\n\nMachine Learning need split for **Train Set** and **Test Set**. We train the model from **Train Set** and use the model for unknowed **Test Set**.\n<hr>\n\n\n### MSE - Mean Square Error\n- Mean of the square difference between the predicted value and true value.\n\n$$ MSE = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat y_i)^2 $$\n\n{% codeblock lang:python %}\n# function mse\t\t- return Mean-Squared Error\ndef mse(y_predicted, y_true):\n    return ((y_predicted - y_true)**2).mean()\n{% endcodeblock %}\n<br>\n\n### RMSE - Root Mean Square Error\n- Square root for Mean of the square difference between the predicted value and true value.\n\n{% codeblock lang:python %}\n# function rmse\t\t- return Root Mean-Squared Error\ndef rmse(y_predicted, y_true):\n    return np.sqrt(((y_predicted - y_true)**2).mean())\n{% endcodeblock %}\n<br>\n\n### RSQ - R Square Score - $ R^2 $\n- $ 0 \\le R^2 \\le 1 $ \n\nThe goal is to make the $ R^2 $ approaching 1 to train the model perfect.\n\n$$ R^2 = 1 - { mse \\over value } $$\n\n{% codeblock lang:python %}\n# function rsq\t\t- return R^2\ndef rsq(y_predicted, y_true):\n    return 1 - ((y_predicted - y_true)**2).mean() / ((y_true - y_true.mean())**2).mean()\n{% endcodeblock %}\n<hr>\n\n### Overfitting\n\nOverfitting means our model fit the trainning set really well, maybe too well. $R^2$ train set way better than $R^2$ test set.\n\ne.g. Train Set $R^2 = 0.97$ , Test Set $R^2 = 0.64$.\n\nSolution: Reduce the complexity of our model from high frequency model to a lower degree polynomial model.\n<br>\n\n### Underfitting\nUnderfitting mean low $R^2$ on both train set and test set. $R^2$ train set and $R^2$ test set are low.\n\ne.g. Train Set $R^2 = 0.64$, Test Set $R^2 = 0.62$.\n\nSolution: We can try more complex model from low degree polynomial model to a higher degree polynomial model.","source":"_posts/begin-machine-learning.md","raw":"---\ntitle: Begin Machine Learning\ncategories: Machine Learning\ndate: 2019-03-30 22:42:55\n---\nWelcome to Machine Learning. Everything is imported from class lectures and backup for future study.\n<br>\n\n### Train_Test_Split:\n```\nInput: X, y, test_size, shuffle, random_state\nOutput: X_train, X_test, y_train, y_test\n```\n- (X, y): features and the target variable.\n- (test_size): between 0 and 1 - how much to allocate to the test set; the rest goes to the train set. \n- (shuffle): if True, shuffle the dataset, otherwise not.\n- (random_state): integer; if None, then results are random, otherwise fixed to a given seed. \n<br>\n\nMachine Learning need split for **Train Set** and **Test Set**. We train the model from **Train Set** and use the model for unknowed **Test Set**.\n<hr>\n\n\n### MSE - Mean Square Error\n- Mean of the square difference between the predicted value and true value.\n\n$$ MSE = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat y_i)^2 $$\n\n{% codeblock lang:python %}\n# function mse\t\t- return Mean-Squared Error\ndef mse(y_predicted, y_true):\n    return ((y_predicted - y_true)**2).mean()\n{% endcodeblock %}\n<br>\n\n### RMSE - Root Mean Square Error\n- Square root for Mean of the square difference between the predicted value and true value.\n\n{% codeblock lang:python %}\n# function rmse\t\t- return Root Mean-Squared Error\ndef rmse(y_predicted, y_true):\n    return np.sqrt(((y_predicted - y_true)**2).mean())\n{% endcodeblock %}\n<br>\n\n### RSQ - R Square Score - $ R^2 $\n- $ 0 \\le R^2 \\le 1 $ \n\nThe goal is to make the $ R^2 $ approaching 1 to train the model perfect.\n\n$$ R^2 = 1 - { mse \\over value } $$\n\n{% codeblock lang:python %}\n# function rsq\t\t- return R^2\ndef rsq(y_predicted, y_true):\n    return 1 - ((y_predicted - y_true)**2).mean() / ((y_true - y_true.mean())**2).mean()\n{% endcodeblock %}\n<hr>\n\n### Overfitting\n\nOverfitting means our model fit the trainning set really well, maybe too well. $R^2$ train set way better than $R^2$ test set.\n\ne.g. Train Set $R^2 = 0.97$ , Test Set $R^2 = 0.64$.\n\nSolution: Reduce the complexity of our model from high frequency model to a lower degree polynomial model.\n<br>\n\n### Underfitting\nUnderfitting mean low $R^2$ on both train set and test set. $R^2$ train set and $R^2$ test set are low.\n\ne.g. Train Set $R^2 = 0.64$, Test Set $R^2 = 0.62$.\n\nSolution: We can try more complex model from low degree polynomial model to a higher degree polynomial model.","slug":"begin-machine-learning","published":1,"updated":"2020-08-14T18:14:41.825Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckduknz560005sniyyjincyr7","content":"<p>Welcome to Machine Learning. Everything is imported from class lectures and backup for future study.<br><br></p>\n<h3 id=\"Train-Test-Split\"><a href=\"#Train-Test-Split\" class=\"headerlink\" title=\"Train_Test_Split:\"></a>Train_Test_Split:</h3><figure class=\"highlight avrasm\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"symbol\">Input:</span> <span class=\"built_in\">X</span>, <span class=\"built_in\">y</span>, test_size, shuffle, random_state</span><br><span class=\"line\"><span class=\"symbol\">Output:</span> X_train, X_test, y_train, y_test</span><br></pre></td></tr></table></figure>\n<ul>\n<li>(X, y): features and the target variable.</li>\n<li>(test_size): between 0 and 1 - how much to allocate to the test set; the rest goes to the train set. </li>\n<li>(shuffle): if True, shuffle the dataset, otherwise not.</li>\n<li>(random_state): integer; if None, then results are random, otherwise fixed to a given seed.<br><br></li>\n</ul>\n<p>Machine Learning need split for <strong>Train Set</strong> and <strong>Test Set</strong>. We train the model from <strong>Train Set</strong> and use the model for unknowed <strong>Test Set</strong>.</p>\n<hr>\n\n\n<h3 id=\"MSE-Mean-Square-Error\"><a href=\"#MSE-Mean-Square-Error\" class=\"headerlink\" title=\"MSE - Mean Square Error\"></a>MSE - Mean Square Error</h3><ul>\n<li>Mean of the square difference between the predicted value and true value.</li>\n</ul>\n<p>$$ MSE = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat y_i)^2 $$</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># function mse\t\t- return Mean-Squared Error</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">mse</span><span class=\"params\">(y_predicted, y_true)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> ((y_predicted - y_true)**<span class=\"number\">2</span>).mean()</span><br></pre></td></tr></table></figure>\n<p><br></p>\n<h3 id=\"RMSE-Root-Mean-Square-Error\"><a href=\"#RMSE-Root-Mean-Square-Error\" class=\"headerlink\" title=\"RMSE - Root Mean Square Error\"></a>RMSE - Root Mean Square Error</h3><ul>\n<li>Square root for Mean of the square difference between the predicted value and true value.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># function rmse\t\t- return Root Mean-Squared Error</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">rmse</span><span class=\"params\">(y_predicted, y_true)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> np.sqrt(((y_predicted - y_true)**<span class=\"number\">2</span>).mean())</span><br></pre></td></tr></table></figure>\n<p><br></p>\n<h3 id=\"RSQ-R-Square-Score-R-2\"><a href=\"#RSQ-R-Square-Score-R-2\" class=\"headerlink\" title=\"RSQ - R Square Score - $ R^2 $\"></a>RSQ - R Square Score - $ R^2 $</h3><ul>\n<li>$ 0 \\le R^2 \\le 1 $ </li>\n</ul>\n<p>The goal is to make the $ R^2 $ approaching 1 to train the model perfect.</p>\n<p>$$ R^2 = 1 - { mse \\over value } $$</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># function rsq\t\t- return R^2</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">rsq</span><span class=\"params\">(y_predicted, y_true)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">1</span> - ((y_predicted - y_true)**<span class=\"number\">2</span>).mean() / ((y_true - y_true.mean())**<span class=\"number\">2</span>).mean()</span><br></pre></td></tr></table></figure>\n<hr>\n\n<h3 id=\"Overfitting\"><a href=\"#Overfitting\" class=\"headerlink\" title=\"Overfitting\"></a>Overfitting</h3><p>Overfitting means our model fit the trainning set really well, maybe too well. $R^2$ train set way better than $R^2$ test set.</p>\n<p>e.g. Train Set $R^2 = 0.97$ , Test Set $R^2 = 0.64$.</p>\n<p>Solution: Reduce the complexity of our model from high frequency model to a lower degree polynomial model.<br><br></p>\n<h3 id=\"Underfitting\"><a href=\"#Underfitting\" class=\"headerlink\" title=\"Underfitting\"></a>Underfitting</h3><p>Underfitting mean low $R^2$ on both train set and test set. $R^2$ train set and $R^2$ test set are low.</p>\n<p>e.g. Train Set $R^2 = 0.64$, Test Set $R^2 = 0.62$.</p>\n<p>Solution: We can try more complex model from low degree polynomial model to a higher degree polynomial model.</p>\n","site":{"data":{}},"excerpt":"","more":"<p>Welcome to Machine Learning. Everything is imported from class lectures and backup for future study.<br><br></p>\n<h3 id=\"Train-Test-Split\"><a href=\"#Train-Test-Split\" class=\"headerlink\" title=\"Train_Test_Split:\"></a>Train_Test_Split:</h3><figure class=\"highlight avrasm\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"symbol\">Input:</span> <span class=\"built_in\">X</span>, <span class=\"built_in\">y</span>, test_size, shuffle, random_state</span><br><span class=\"line\"><span class=\"symbol\">Output:</span> X_train, X_test, y_train, y_test</span><br></pre></td></tr></table></figure>\n<ul>\n<li>(X, y): features and the target variable.</li>\n<li>(test_size): between 0 and 1 - how much to allocate to the test set; the rest goes to the train set. </li>\n<li>(shuffle): if True, shuffle the dataset, otherwise not.</li>\n<li>(random_state): integer; if None, then results are random, otherwise fixed to a given seed.<br><br></li>\n</ul>\n<p>Machine Learning need split for <strong>Train Set</strong> and <strong>Test Set</strong>. We train the model from <strong>Train Set</strong> and use the model for unknowed <strong>Test Set</strong>.</p>\n<hr>\n\n\n<h3 id=\"MSE-Mean-Square-Error\"><a href=\"#MSE-Mean-Square-Error\" class=\"headerlink\" title=\"MSE - Mean Square Error\"></a>MSE - Mean Square Error</h3><ul>\n<li>Mean of the square difference between the predicted value and true value.</li>\n</ul>\n<p>$$ MSE = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat y_i)^2 $$</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># function mse\t\t- return Mean-Squared Error</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">mse</span><span class=\"params\">(y_predicted, y_true)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> ((y_predicted - y_true)**<span class=\"number\">2</span>).mean()</span><br></pre></td></tr></table></figure>\n<p><br></p>\n<h3 id=\"RMSE-Root-Mean-Square-Error\"><a href=\"#RMSE-Root-Mean-Square-Error\" class=\"headerlink\" title=\"RMSE - Root Mean Square Error\"></a>RMSE - Root Mean Square Error</h3><ul>\n<li>Square root for Mean of the square difference between the predicted value and true value.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># function rmse\t\t- return Root Mean-Squared Error</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">rmse</span><span class=\"params\">(y_predicted, y_true)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> np.sqrt(((y_predicted - y_true)**<span class=\"number\">2</span>).mean())</span><br></pre></td></tr></table></figure>\n<p><br></p>\n<h3 id=\"RSQ-R-Square-Score-R-2\"><a href=\"#RSQ-R-Square-Score-R-2\" class=\"headerlink\" title=\"RSQ - R Square Score - $ R^2 $\"></a>RSQ - R Square Score - $ R^2 $</h3><ul>\n<li>$ 0 \\le R^2 \\le 1 $ </li>\n</ul>\n<p>The goal is to make the $ R^2 $ approaching 1 to train the model perfect.</p>\n<p>$$ R^2 = 1 - { mse \\over value } $$</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># function rsq\t\t- return R^2</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">rsq</span><span class=\"params\">(y_predicted, y_true)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">1</span> - ((y_predicted - y_true)**<span class=\"number\">2</span>).mean() / ((y_true - y_true.mean())**<span class=\"number\">2</span>).mean()</span><br></pre></td></tr></table></figure>\n<hr>\n\n<h3 id=\"Overfitting\"><a href=\"#Overfitting\" class=\"headerlink\" title=\"Overfitting\"></a>Overfitting</h3><p>Overfitting means our model fit the trainning set really well, maybe too well. $R^2$ train set way better than $R^2$ test set.</p>\n<p>e.g. Train Set $R^2 = 0.97$ , Test Set $R^2 = 0.64$.</p>\n<p>Solution: Reduce the complexity of our model from high frequency model to a lower degree polynomial model.<br><br></p>\n<h3 id=\"Underfitting\"><a href=\"#Underfitting\" class=\"headerlink\" title=\"Underfitting\"></a>Underfitting</h3><p>Underfitting mean low $R^2$ on both train set and test set. $R^2$ train set and $R^2$ test set are low.</p>\n<p>e.g. Train Set $R^2 = 0.64$, Test Set $R^2 = 0.62$.</p>\n<p>Solution: We can try more complex model from low degree polynomial model to a higher degree polynomial model.</p>\n"},{"title":"Joker","date":"2019-11-16T08:57:34.000Z","_content":"\n每个人的心中都活着一个Joker\n用这厌世的态度与社会抗衡\n这世界本就没有绝对的公平\n笑只是你的保护色\n\n在不该笑的场合\n笑的如此哗众取宠\n叛逆也是一种生活方式\n双手撕裂嘴唇来给予笑容\n\n<br>\n\n<center><img src=\"{% asset_path joker-1-smile.jpg %}\"  width=\"75%\"></center>\n\n<br>\n\n{% blockquote %}\nI just hope my death makes more cents than my life.\n{% endblockquote %}\n\n<br>\n\n生活本就是一场悲剧\n只不过每个人的悲惨程度不同而已\n本想要笑着对待他人\n换来的也只有不屑\n\n<br>\n\n<center><img src=\"{% asset_path joker-2-bus.jpg %}\"  width=\"75%\"></center>\n\n<br>\n\n{% blockquote %}\nForgive my Laughter: \nI have a Condition.\n\n-MORE ON BACK\n{% endblockquote %}\n\n<br>\n\n<center><img src=\"{% asset_path joker-3-card.png %}\"  width=\"75%\"></center>\n\n<br>\n\n在不合适的场合 做了不合适的事\n他人歧视的眼光中透露着厌恶\n真实的想法和心情只能深藏心底\n别让任何人看穿你脆弱的一面\n用尖锐的笑声\n伪装合群的瞬间\n\n<br>\n\n<center><img src=\"{% asset_path joker-4-stairs.jpg %}\"  width=\"75%\"></center>\n\n<br>\n\n在无人的角落\n拖动着疲惫的身躯\n生活的无助\n每天都在日复一日的重复着\n内心的挣扎\n不如就此结束生命做个了断\n\n<br>\n\n<center><img src=\"{% asset_path joker-5-notebook.png %}\"  width=\"75%\"></center>\n\n<br>\n\n观察着身边的事物\n寻找着快乐的间刻\n自己试问自己该如何融入\n得到却是无法改变的结果\n\n本想对这世界微笑 **Put On a HAPPY FACE**\n却被世界对立仇视到 **Don’t ~~Forget To~~ Smile!**\n\n<br>\n\n<center><img src=\"{% asset_path joker-6-dont-smile.png %}\"  width=\"75%\"></center>\n\n<br>\n\n从未有人在乎过自己\n以至自己都怀疑自己是否存在\n从未有人专心聆听自己\n内心有的一切都只是负面情绪\n你说这就是人生\n无法改变的命运\n\n<br>\n\n{% blockquote %}\nAll I have are negative thoughts, but you don’t listen anyway. I said.\nFor my whole life, I didn’t know if I even really existed. But I do, and people are starting to notice!\n{% endblockquote %}\n\n<br>\n\n放映着卓别林的喜剧\n同步着正上演的悲剧\n看似笑话般的剧情\n诠释着黑色的幽默\n\n<br>\n\n<center><img src=\"{% asset_path joker-7-gun.png %}\"  width=\"75%\"></center>\n\n<br>\n\n当谎言一个接一个被戳破\n美好的陪伴都只曾是幻想\n现实残酷的鞭打着自己\n以快乐作为自己的名字\n一生却从未一刻快乐过\n\n<br>\n\n{% blockquote %}\nI used to think that my life was a tragedy, but now I realize, it’s a fucking comedy.\n{% endblockquote %}\n\n<br>\n\n对一切毫无留恋\n又何以畏惧死亡\n嗜杀也不过为偿还曾经的羞辱\n用生命去换取最后的一丝尊严\n\n<br>\n\n<center><img src=\"{% asset_path joker-8-blood-smile.png %}\"  width=\"75%\"></center>\n","source":"_posts/joker.md","raw":"---\ntitle: Joker\ncategories: Life\ndate: 2019-11-16 00:57:34\n---\n\n每个人的心中都活着一个Joker\n用这厌世的态度与社会抗衡\n这世界本就没有绝对的公平\n笑只是你的保护色\n\n在不该笑的场合\n笑的如此哗众取宠\n叛逆也是一种生活方式\n双手撕裂嘴唇来给予笑容\n\n<br>\n\n<center><img src=\"{% asset_path joker-1-smile.jpg %}\"  width=\"75%\"></center>\n\n<br>\n\n{% blockquote %}\nI just hope my death makes more cents than my life.\n{% endblockquote %}\n\n<br>\n\n生活本就是一场悲剧\n只不过每个人的悲惨程度不同而已\n本想要笑着对待他人\n换来的也只有不屑\n\n<br>\n\n<center><img src=\"{% asset_path joker-2-bus.jpg %}\"  width=\"75%\"></center>\n\n<br>\n\n{% blockquote %}\nForgive my Laughter: \nI have a Condition.\n\n-MORE ON BACK\n{% endblockquote %}\n\n<br>\n\n<center><img src=\"{% asset_path joker-3-card.png %}\"  width=\"75%\"></center>\n\n<br>\n\n在不合适的场合 做了不合适的事\n他人歧视的眼光中透露着厌恶\n真实的想法和心情只能深藏心底\n别让任何人看穿你脆弱的一面\n用尖锐的笑声\n伪装合群的瞬间\n\n<br>\n\n<center><img src=\"{% asset_path joker-4-stairs.jpg %}\"  width=\"75%\"></center>\n\n<br>\n\n在无人的角落\n拖动着疲惫的身躯\n生活的无助\n每天都在日复一日的重复着\n内心的挣扎\n不如就此结束生命做个了断\n\n<br>\n\n<center><img src=\"{% asset_path joker-5-notebook.png %}\"  width=\"75%\"></center>\n\n<br>\n\n观察着身边的事物\n寻找着快乐的间刻\n自己试问自己该如何融入\n得到却是无法改变的结果\n\n本想对这世界微笑 **Put On a HAPPY FACE**\n却被世界对立仇视到 **Don’t ~~Forget To~~ Smile!**\n\n<br>\n\n<center><img src=\"{% asset_path joker-6-dont-smile.png %}\"  width=\"75%\"></center>\n\n<br>\n\n从未有人在乎过自己\n以至自己都怀疑自己是否存在\n从未有人专心聆听自己\n内心有的一切都只是负面情绪\n你说这就是人生\n无法改变的命运\n\n<br>\n\n{% blockquote %}\nAll I have are negative thoughts, but you don’t listen anyway. I said.\nFor my whole life, I didn’t know if I even really existed. But I do, and people are starting to notice!\n{% endblockquote %}\n\n<br>\n\n放映着卓别林的喜剧\n同步着正上演的悲剧\n看似笑话般的剧情\n诠释着黑色的幽默\n\n<br>\n\n<center><img src=\"{% asset_path joker-7-gun.png %}\"  width=\"75%\"></center>\n\n<br>\n\n当谎言一个接一个被戳破\n美好的陪伴都只曾是幻想\n现实残酷的鞭打着自己\n以快乐作为自己的名字\n一生却从未一刻快乐过\n\n<br>\n\n{% blockquote %}\nI used to think that my life was a tragedy, but now I realize, it’s a fucking comedy.\n{% endblockquote %}\n\n<br>\n\n对一切毫无留恋\n又何以畏惧死亡\n嗜杀也不过为偿还曾经的羞辱\n用生命去换取最后的一丝尊严\n\n<br>\n\n<center><img src=\"{% asset_path joker-8-blood-smile.png %}\"  width=\"75%\"></center>\n","slug":"joker","published":1,"updated":"2020-08-14T18:14:41.828Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckduknz570006sniygvdacswg","content":"<p>每个人的心中都活着一个Joker<br>用这厌世的态度与社会抗衡<br>这世界本就没有绝对的公平<br>笑只是你的保护色</p>\n<p>在不该笑的场合<br>笑的如此哗众取宠<br>叛逆也是一种生活方式<br>双手撕裂嘴唇来给予笑容</p>\n<p><br></p>\n<center><img src=\"/Life/joker/joker-1-smile.jpg\" width=\"75%\"></center>\n\n<p><br></p>\n<blockquote><p>I just hope my death makes more cents than my life.</p>\n</blockquote>\n<p><br></p>\n<p>生活本就是一场悲剧<br>只不过每个人的悲惨程度不同而已<br>本想要笑着对待他人<br>换来的也只有不屑</p>\n<p><br></p>\n<center><img src=\"/Life/joker/joker-2-bus.jpg\" width=\"75%\"></center>\n\n<p><br></p>\n<blockquote><p>Forgive my Laughter:<br>I have a Condition.</p>\n<p>-MORE ON BACK</p>\n</blockquote>\n<p><br></p>\n<center><img src=\"/Life/joker/joker-3-card.png\" width=\"75%\"></center>\n\n<p><br></p>\n<p>在不合适的场合 做了不合适的事<br>他人歧视的眼光中透露着厌恶<br>真实的想法和心情只能深藏心底<br>别让任何人看穿你脆弱的一面<br>用尖锐的笑声<br>伪装合群的瞬间</p>\n<p><br></p>\n<center><img src=\"/Life/joker/joker-4-stairs.jpg\" width=\"75%\"></center>\n\n<p><br></p>\n<p>在无人的角落<br>拖动着疲惫的身躯<br>生活的无助<br>每天都在日复一日的重复着<br>内心的挣扎<br>不如就此结束生命做个了断</p>\n<p><br></p>\n<center><img src=\"/Life/joker/joker-5-notebook.png\" width=\"75%\"></center>\n\n<p><br></p>\n<p>观察着身边的事物<br>寻找着快乐的间刻<br>自己试问自己该如何融入<br>得到却是无法改变的结果</p>\n<p>本想对这世界微笑 <strong>Put On a HAPPY FACE</strong><br>却被世界对立仇视到 <strong>Don’t <del>Forget To</del> Smile!</strong></p>\n<p><br></p>\n<center><img src=\"/Life/joker/joker-6-dont-smile.png\" width=\"75%\"></center>\n\n<p><br></p>\n<p>从未有人在乎过自己<br>以至自己都怀疑自己是否存在<br>从未有人专心聆听自己<br>内心有的一切都只是负面情绪<br>你说这就是人生<br>无法改变的命运</p>\n<p><br></p>\n<blockquote><p>All I have are negative thoughts, but you don’t listen anyway. I said.<br>For my whole life, I didn’t know if I even really existed. But I do, and people are starting to notice!</p>\n</blockquote>\n<p><br></p>\n<p>放映着卓别林的喜剧<br>同步着正上演的悲剧<br>看似笑话般的剧情<br>诠释着黑色的幽默</p>\n<p><br></p>\n<center><img src=\"/Life/joker/joker-7-gun.png\" width=\"75%\"></center>\n\n<p><br></p>\n<p>当谎言一个接一个被戳破<br>美好的陪伴都只曾是幻想<br>现实残酷的鞭打着自己<br>以快乐作为自己的名字<br>一生却从未一刻快乐过</p>\n<p><br></p>\n<blockquote><p>I used to think that my life was a tragedy, but now I realize, it’s a fucking comedy.</p>\n</blockquote>\n<p><br></p>\n<p>对一切毫无留恋<br>又何以畏惧死亡<br>嗜杀也不过为偿还曾经的羞辱<br>用生命去换取最后的一丝尊严</p>\n<p><br></p>\n<center><img src=\"/Life/joker/joker-8-blood-smile.png\" width=\"75%\"></center>\n","site":{"data":{}},"excerpt":"","more":"<p>每个人的心中都活着一个Joker<br>用这厌世的态度与社会抗衡<br>这世界本就没有绝对的公平<br>笑只是你的保护色</p>\n<p>在不该笑的场合<br>笑的如此哗众取宠<br>叛逆也是一种生活方式<br>双手撕裂嘴唇来给予笑容</p>\n<p><br></p>\n<center><img src=\"/Life/joker/joker-1-smile.jpg\" width=\"75%\"></center>\n\n<p><br></p>\n<blockquote><p>I just hope my death makes more cents than my life.</p>\n</blockquote>\n<p><br></p>\n<p>生活本就是一场悲剧<br>只不过每个人的悲惨程度不同而已<br>本想要笑着对待他人<br>换来的也只有不屑</p>\n<p><br></p>\n<center><img src=\"/Life/joker/joker-2-bus.jpg\" width=\"75%\"></center>\n\n<p><br></p>\n<blockquote><p>Forgive my Laughter:<br>I have a Condition.</p>\n<p>-MORE ON BACK</p>\n</blockquote>\n<p><br></p>\n<center><img src=\"/Life/joker/joker-3-card.png\" width=\"75%\"></center>\n\n<p><br></p>\n<p>在不合适的场合 做了不合适的事<br>他人歧视的眼光中透露着厌恶<br>真实的想法和心情只能深藏心底<br>别让任何人看穿你脆弱的一面<br>用尖锐的笑声<br>伪装合群的瞬间</p>\n<p><br></p>\n<center><img src=\"/Life/joker/joker-4-stairs.jpg\" width=\"75%\"></center>\n\n<p><br></p>\n<p>在无人的角落<br>拖动着疲惫的身躯<br>生活的无助<br>每天都在日复一日的重复着<br>内心的挣扎<br>不如就此结束生命做个了断</p>\n<p><br></p>\n<center><img src=\"/Life/joker/joker-5-notebook.png\" width=\"75%\"></center>\n\n<p><br></p>\n<p>观察着身边的事物<br>寻找着快乐的间刻<br>自己试问自己该如何融入<br>得到却是无法改变的结果</p>\n<p>本想对这世界微笑 <strong>Put On a HAPPY FACE</strong><br>却被世界对立仇视到 <strong>Don’t <del>Forget To</del> Smile!</strong></p>\n<p><br></p>\n<center><img src=\"/Life/joker/joker-6-dont-smile.png\" width=\"75%\"></center>\n\n<p><br></p>\n<p>从未有人在乎过自己<br>以至自己都怀疑自己是否存在<br>从未有人专心聆听自己<br>内心有的一切都只是负面情绪<br>你说这就是人生<br>无法改变的命运</p>\n<p><br></p>\n<blockquote><p>All I have are negative thoughts, but you don’t listen anyway. I said.<br>For my whole life, I didn’t know if I even really existed. But I do, and people are starting to notice!</p>\n</blockquote>\n<p><br></p>\n<p>放映着卓别林的喜剧<br>同步着正上演的悲剧<br>看似笑话般的剧情<br>诠释着黑色的幽默</p>\n<p><br></p>\n<center><img src=\"/Life/joker/joker-7-gun.png\" width=\"75%\"></center>\n\n<p><br></p>\n<p>当谎言一个接一个被戳破<br>美好的陪伴都只曾是幻想<br>现实残酷的鞭打着自己<br>以快乐作为自己的名字<br>一生却从未一刻快乐过</p>\n<p><br></p>\n<blockquote><p>I used to think that my life was a tragedy, but now I realize, it’s a fucking comedy.</p>\n</blockquote>\n<p><br></p>\n<p>对一切毫无留恋<br>又何以畏惧死亡<br>嗜杀也不过为偿还曾经的羞辱<br>用生命去换取最后的一丝尊严</p>\n<p><br></p>\n<center><img src=\"/Life/joker/joker-8-blood-smile.png\" width=\"75%\"></center>\n"},{"title":"Linear Regression","date":"2019-04-23T09:26:55.000Z","_content":"\n### One dimensional Case\nh is the output for prediction if give x-value, and y is output, h is predict value\n\n$$ h=ax+b $$\n\n#### Loss Function\n\n$$ Loss = L(a, b) $$\n\n\nThe goal is to minize the loss function, the value a and b define the predict function. When the function achive the lowest value will be a good loss function.\n\n$$ Loss = MSE = \\frac{1}{N} \\sum_{i=1}^N (y_i - h_i)^2 $$\n\n\nStart from a = 0, b = 0, then update at each iteration. Using derivative to find which direction we need to change. $ \\frac{\\partial L}{\\partial a} = 0$ and $ \\frac{\\partial L}{\\partial b} = 0 $\n\nTherefore, from Loss function we can get:\n$$ L(a, b)= \\frac{1}{N} \\sum_{i=1}^N (y_i - a x_i - b)^2 $$\n\nGiving learning rate: $ \\alpha $ = learning rate \n\n$$ \\frac{\\partial L}{\\partial a} = \\frac{1}{N} \\sum_{i=1}^N (h_i - y_i) * x_i $$\n\n$$ \\frac{\\partial L}{\\partial b} = \\frac{1}{N} \\sum_{i=1}^N (h_i - y_i) $$\n\nFor a and b updating :\n\na = $ a - \\frac{\\partial L}{\\partial a} * \\alpha $\n\nb = $ b - \\frac{\\partial L}{\\partial b} * \\alpha $\n \nAfter all, when we find the minimum value for Loss function, then it will be the a & b for the linear regression.\n\n#### Excel Example\n\n|  x  |  y  | y_pred | y_pred - y | (y_pred-y) * x |\n| :-: | :-: | :-: | :-: | :-: |\n| [0] |[2.3]|  0  |-2.3 |  0  |\n|[1.11]|[2.7]|  0  |-2.7 |-2.997|\n|[2.8]|[7.1]|  0  |-7.1 |-19.88|\n|**Average**| | |-4.0333|-7.62567|\n\n|   |Initial|Example|Equation|\n| :-: | :-: | :-: | :-: |\n|**a**|0|0 - (0.1 * -7.62567)|a - [LR x Average((y_pred-y) * x)]|\n|**b**|0|0 - (0.1 * -4.0333)|b - [LR x Average((y_pred_y)]|\n|**Learning rate (LR)**|0.1|\n|**Projected**|0|$\\sqrt{(0)^2 + (0)^2}$|$\\sqrt{(a)^2 + (b)^2}$|\n|**Loss**|62.99|$\\frac{(-2.3)^2 + (-2.7)^2 + (-7.1)^2}{3} $|$\\frac{1}{N}\\sum_{i=1}^N (y\\\\_pred -y)^2$|\n\n<hr>\n\n### Multi-Dimensional Case\nIn multi-dimensional case, the a will be replace by $w_i$, and the formular is same as the one dimensional case.\n\n$$ h = \\sum_{i=1}^N (w_i * x_i) + b $$\n\n\n#### Normal Distribution\n\nInput **X** change to **X_norm**\nInput **Y** change to **y_norm**\n\nFor each feature of X input, We have to calculate **mean** and **STD** (Standard Deviation). And Using those two value to get **X_norm** and **y_norm**.\n\nFrom real value change to normal distribution:\n\n$$ X\\\\_NORM = \\frac{(X - \\overline X)}{STD} $$\n$$ y\\\\_NORM = \\frac{(y - \\overline y)}{STD} $$\n\nBecuase we using normal distribution to get **weight** therefore, when we get back y predict value we need using **mean** and **STD** to change back real value:\n\n$$ y = y\\\\_Predict \\cdot y\\\\_STD + \\overline y $$\n\n#### Batch\nWith batch **stochastic gradient descent**, we'd need fewer iterations and improve the **LOSS** value faster. \n\nThink of iteration as epoch - you pass over the entire dataset. So if you split into batches, you’d go over all batches and this will be one iteration.\n\n{% codeblock lang:python %}\nbatch_size = 4\nbatches = int( X.shape[0] / self.batch_size )\n\nfor i in range(batches):\n\tstart = i * batch_size\n\tend = start + batch_size\n\tX = X[start:end]\n\ty = y[start:end]\n{% endcodeblock %}\n\n","source":"_posts/linear-regression.md","raw":"---\ntitle: Linear Regression\ncategories: Machine Learning\ndate: 2019-04-23 02:26:55\n---\n\n### One dimensional Case\nh is the output for prediction if give x-value, and y is output, h is predict value\n\n$$ h=ax+b $$\n\n#### Loss Function\n\n$$ Loss = L(a, b) $$\n\n\nThe goal is to minize the loss function, the value a and b define the predict function. When the function achive the lowest value will be a good loss function.\n\n$$ Loss = MSE = \\frac{1}{N} \\sum_{i=1}^N (y_i - h_i)^2 $$\n\n\nStart from a = 0, b = 0, then update at each iteration. Using derivative to find which direction we need to change. $ \\frac{\\partial L}{\\partial a} = 0$ and $ \\frac{\\partial L}{\\partial b} = 0 $\n\nTherefore, from Loss function we can get:\n$$ L(a, b)= \\frac{1}{N} \\sum_{i=1}^N (y_i - a x_i - b)^2 $$\n\nGiving learning rate: $ \\alpha $ = learning rate \n\n$$ \\frac{\\partial L}{\\partial a} = \\frac{1}{N} \\sum_{i=1}^N (h_i - y_i) * x_i $$\n\n$$ \\frac{\\partial L}{\\partial b} = \\frac{1}{N} \\sum_{i=1}^N (h_i - y_i) $$\n\nFor a and b updating :\n\na = $ a - \\frac{\\partial L}{\\partial a} * \\alpha $\n\nb = $ b - \\frac{\\partial L}{\\partial b} * \\alpha $\n \nAfter all, when we find the minimum value for Loss function, then it will be the a & b for the linear regression.\n\n#### Excel Example\n\n|  x  |  y  | y_pred | y_pred - y | (y_pred-y) * x |\n| :-: | :-: | :-: | :-: | :-: |\n| [0] |[2.3]|  0  |-2.3 |  0  |\n|[1.11]|[2.7]|  0  |-2.7 |-2.997|\n|[2.8]|[7.1]|  0  |-7.1 |-19.88|\n|**Average**| | |-4.0333|-7.62567|\n\n|   |Initial|Example|Equation|\n| :-: | :-: | :-: | :-: |\n|**a**|0|0 - (0.1 * -7.62567)|a - [LR x Average((y_pred-y) * x)]|\n|**b**|0|0 - (0.1 * -4.0333)|b - [LR x Average((y_pred_y)]|\n|**Learning rate (LR)**|0.1|\n|**Projected**|0|$\\sqrt{(0)^2 + (0)^2}$|$\\sqrt{(a)^2 + (b)^2}$|\n|**Loss**|62.99|$\\frac{(-2.3)^2 + (-2.7)^2 + (-7.1)^2}{3} $|$\\frac{1}{N}\\sum_{i=1}^N (y\\\\_pred -y)^2$|\n\n<hr>\n\n### Multi-Dimensional Case\nIn multi-dimensional case, the a will be replace by $w_i$, and the formular is same as the one dimensional case.\n\n$$ h = \\sum_{i=1}^N (w_i * x_i) + b $$\n\n\n#### Normal Distribution\n\nInput **X** change to **X_norm**\nInput **Y** change to **y_norm**\n\nFor each feature of X input, We have to calculate **mean** and **STD** (Standard Deviation). And Using those two value to get **X_norm** and **y_norm**.\n\nFrom real value change to normal distribution:\n\n$$ X\\\\_NORM = \\frac{(X - \\overline X)}{STD} $$\n$$ y\\\\_NORM = \\frac{(y - \\overline y)}{STD} $$\n\nBecuase we using normal distribution to get **weight** therefore, when we get back y predict value we need using **mean** and **STD** to change back real value:\n\n$$ y = y\\\\_Predict \\cdot y\\\\_STD + \\overline y $$\n\n#### Batch\nWith batch **stochastic gradient descent**, we'd need fewer iterations and improve the **LOSS** value faster. \n\nThink of iteration as epoch - you pass over the entire dataset. So if you split into batches, you’d go over all batches and this will be one iteration.\n\n{% codeblock lang:python %}\nbatch_size = 4\nbatches = int( X.shape[0] / self.batch_size )\n\nfor i in range(batches):\n\tstart = i * batch_size\n\tend = start + batch_size\n\tX = X[start:end]\n\ty = y[start:end]\n{% endcodeblock %}\n\n","slug":"linear-regression","published":1,"updated":"2020-08-14T18:14:41.877Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckduknz580007sniyxnoqub7f","content":"<h3 id=\"One-dimensional-Case\"><a href=\"#One-dimensional-Case\" class=\"headerlink\" title=\"One dimensional Case\"></a>One dimensional Case</h3><p>h is the output for prediction if give x-value, and y is output, h is predict value</p>\n<p>$$ h=ax+b $$</p>\n<h4 id=\"Loss-Function\"><a href=\"#Loss-Function\" class=\"headerlink\" title=\"Loss Function\"></a>Loss Function</h4><p>$$ Loss = L(a, b) $$</p>\n<p>The goal is to minize the loss function, the value a and b define the predict function. When the function achive the lowest value will be a good loss function.</p>\n<p>$$ Loss = MSE = \\frac{1}{N} \\sum_{i=1}^N (y_i - h_i)^2 $$</p>\n<p>Start from a = 0, b = 0, then update at each iteration. Using derivative to find which direction we need to change. $ \\frac{\\partial L}{\\partial a} = 0$ and $ \\frac{\\partial L}{\\partial b} = 0 $</p>\n<p>Therefore, from Loss function we can get:<br>$$ L(a, b)= \\frac{1}{N} \\sum_{i=1}^N (y_i - a x_i - b)^2 $$</p>\n<p>Giving learning rate: $ \\alpha $ = learning rate </p>\n<p>$$ \\frac{\\partial L}{\\partial a} = \\frac{1}{N} \\sum_{i=1}^N (h_i - y_i) * x_i $$</p>\n<p>$$ \\frac{\\partial L}{\\partial b} = \\frac{1}{N} \\sum_{i=1}^N (h_i - y_i) $$</p>\n<p>For a and b updating :</p>\n<p>a = $ a - \\frac{\\partial L}{\\partial a} * \\alpha $</p>\n<p>b = $ b - \\frac{\\partial L}{\\partial b} * \\alpha $</p>\n<p>After all, when we find the minimum value for Loss function, then it will be the a &amp; b for the linear regression.</p>\n<h4 id=\"Excel-Example\"><a href=\"#Excel-Example\" class=\"headerlink\" title=\"Excel Example\"></a>Excel Example</h4><table>\n<thead>\n<tr>\n<th style=\"text-align:center\">x</th>\n<th style=\"text-align:center\">y</th>\n<th style=\"text-align:center\">y_pred</th>\n<th style=\"text-align:center\">y_pred - y</th>\n<th style=\"text-align:center\">(y_pred-y) * x</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">[0]</td>\n<td style=\"text-align:center\">[2.3]</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">-2.3</td>\n<td style=\"text-align:center\">0</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">[1.11]</td>\n<td style=\"text-align:center\">[2.7]</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">-2.7</td>\n<td style=\"text-align:center\">-2.997</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">[2.8]</td>\n<td style=\"text-align:center\">[7.1]</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">-7.1</td>\n<td style=\"text-align:center\">-19.88</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><strong>Average</strong></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">-4.0333</td>\n<td style=\"text-align:center\">-7.62567</td>\n</tr>\n</tbody>\n</table>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\"></th>\n<th style=\"text-align:center\">Initial</th>\n<th style=\"text-align:center\">Example</th>\n<th style=\"text-align:center\">Equation</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\"><strong>a</strong></td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">0 - (0.1 * -7.62567)</td>\n<td style=\"text-align:center\">a - [LR x Average((y_pred-y) * x)]</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><strong>b</strong></td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">0 - (0.1 * -4.0333)</td>\n<td style=\"text-align:center\">b - [LR x Average((y_pred_y)]</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><strong>Learning rate (LR)</strong></td>\n<td style=\"text-align:center\">0.1</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><strong>Projected</strong></td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">$\\sqrt{(0)^2 + (0)^2}$</td>\n<td style=\"text-align:center\">$\\sqrt{(a)^2 + (b)^2}$</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><strong>Loss</strong></td>\n<td style=\"text-align:center\">62.99</td>\n<td style=\"text-align:center\">$\\frac{(-2.3)^2 + (-2.7)^2 + (-7.1)^2}{3} $</td>\n<td style=\"text-align:center\">$\\frac{1}{N}\\sum_{i=1}^N (y\\_pred -y)^2$</td>\n</tr>\n</tbody>\n</table>\n<hr>\n\n<h3 id=\"Multi-Dimensional-Case\"><a href=\"#Multi-Dimensional-Case\" class=\"headerlink\" title=\"Multi-Dimensional Case\"></a>Multi-Dimensional Case</h3><p>In multi-dimensional case, the a will be replace by $w_i$, and the formular is same as the one dimensional case.</p>\n<p>$$ h = \\sum_{i=1}^N (w_i * x_i) + b $$</p>\n<h4 id=\"Normal-Distribution\"><a href=\"#Normal-Distribution\" class=\"headerlink\" title=\"Normal Distribution\"></a>Normal Distribution</h4><p>Input <strong>X</strong> change to <strong>X_norm</strong><br>Input <strong>Y</strong> change to <strong>y_norm</strong></p>\n<p>For each feature of X input, We have to calculate <strong>mean</strong> and <strong>STD</strong> (Standard Deviation). And Using those two value to get <strong>X_norm</strong> and <strong>y_norm</strong>.</p>\n<p>From real value change to normal distribution:</p>\n<p>$$ X\\_NORM = \\frac{(X - \\overline X)}{STD} $$<br>$$ y\\_NORM = \\frac{(y - \\overline y)}{STD} $$</p>\n<p>Becuase we using normal distribution to get <strong>weight</strong> therefore, when we get back y predict value we need using <strong>mean</strong> and <strong>STD</strong> to change back real value:</p>\n<p>$$ y = y\\_Predict \\cdot y\\_STD + \\overline y $$</p>\n<h4 id=\"Batch\"><a href=\"#Batch\" class=\"headerlink\" title=\"Batch\"></a>Batch</h4><p>With batch <strong>stochastic gradient descent</strong>, we’d need fewer iterations and improve the <strong>LOSS</strong> value faster. </p>\n<p>Think of iteration as epoch - you pass over the entire dataset. So if you split into batches, you’d go over all batches and this will be one iteration.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">batch_size = <span class=\"number\">4</span></span><br><span class=\"line\">batches = int( X.shape[<span class=\"number\">0</span>] / self.batch_size )</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(batches):</span><br><span class=\"line\">\tstart = i * batch_size</span><br><span class=\"line\">\tend = start + batch_size</span><br><span class=\"line\">\tX = X[start:end]</span><br><span class=\"line\">\ty = y[start:end]</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"One-dimensional-Case\"><a href=\"#One-dimensional-Case\" class=\"headerlink\" title=\"One dimensional Case\"></a>One dimensional Case</h3><p>h is the output for prediction if give x-value, and y is output, h is predict value</p>\n<p>$$ h=ax+b $$</p>\n<h4 id=\"Loss-Function\"><a href=\"#Loss-Function\" class=\"headerlink\" title=\"Loss Function\"></a>Loss Function</h4><p>$$ Loss = L(a, b) $$</p>\n<p>The goal is to minize the loss function, the value a and b define the predict function. When the function achive the lowest value will be a good loss function.</p>\n<p>$$ Loss = MSE = \\frac{1}{N} \\sum_{i=1}^N (y_i - h_i)^2 $$</p>\n<p>Start from a = 0, b = 0, then update at each iteration. Using derivative to find which direction we need to change. $ \\frac{\\partial L}{\\partial a} = 0$ and $ \\frac{\\partial L}{\\partial b} = 0 $</p>\n<p>Therefore, from Loss function we can get:<br>$$ L(a, b)= \\frac{1}{N} \\sum_{i=1}^N (y_i - a x_i - b)^2 $$</p>\n<p>Giving learning rate: $ \\alpha $ = learning rate </p>\n<p>$$ \\frac{\\partial L}{\\partial a} = \\frac{1}{N} \\sum_{i=1}^N (h_i - y_i) * x_i $$</p>\n<p>$$ \\frac{\\partial L}{\\partial b} = \\frac{1}{N} \\sum_{i=1}^N (h_i - y_i) $$</p>\n<p>For a and b updating :</p>\n<p>a = $ a - \\frac{\\partial L}{\\partial a} * \\alpha $</p>\n<p>b = $ b - \\frac{\\partial L}{\\partial b} * \\alpha $</p>\n<p>After all, when we find the minimum value for Loss function, then it will be the a &amp; b for the linear regression.</p>\n<h4 id=\"Excel-Example\"><a href=\"#Excel-Example\" class=\"headerlink\" title=\"Excel Example\"></a>Excel Example</h4><table>\n<thead>\n<tr>\n<th style=\"text-align:center\">x</th>\n<th style=\"text-align:center\">y</th>\n<th style=\"text-align:center\">y_pred</th>\n<th style=\"text-align:center\">y_pred - y</th>\n<th style=\"text-align:center\">(y_pred-y) * x</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">[0]</td>\n<td style=\"text-align:center\">[2.3]</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">-2.3</td>\n<td style=\"text-align:center\">0</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">[1.11]</td>\n<td style=\"text-align:center\">[2.7]</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">-2.7</td>\n<td style=\"text-align:center\">-2.997</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">[2.8]</td>\n<td style=\"text-align:center\">[7.1]</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">-7.1</td>\n<td style=\"text-align:center\">-19.88</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><strong>Average</strong></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">-4.0333</td>\n<td style=\"text-align:center\">-7.62567</td>\n</tr>\n</tbody>\n</table>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\"></th>\n<th style=\"text-align:center\">Initial</th>\n<th style=\"text-align:center\">Example</th>\n<th style=\"text-align:center\">Equation</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\"><strong>a</strong></td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">0 - (0.1 * -7.62567)</td>\n<td style=\"text-align:center\">a - [LR x Average((y_pred-y) * x)]</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><strong>b</strong></td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">0 - (0.1 * -4.0333)</td>\n<td style=\"text-align:center\">b - [LR x Average((y_pred_y)]</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><strong>Learning rate (LR)</strong></td>\n<td style=\"text-align:center\">0.1</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><strong>Projected</strong></td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">$\\sqrt{(0)^2 + (0)^2}$</td>\n<td style=\"text-align:center\">$\\sqrt{(a)^2 + (b)^2}$</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><strong>Loss</strong></td>\n<td style=\"text-align:center\">62.99</td>\n<td style=\"text-align:center\">$\\frac{(-2.3)^2 + (-2.7)^2 + (-7.1)^2}{3} $</td>\n<td style=\"text-align:center\">$\\frac{1}{N}\\sum_{i=1}^N (y\\_pred -y)^2$</td>\n</tr>\n</tbody>\n</table>\n<hr>\n\n<h3 id=\"Multi-Dimensional-Case\"><a href=\"#Multi-Dimensional-Case\" class=\"headerlink\" title=\"Multi-Dimensional Case\"></a>Multi-Dimensional Case</h3><p>In multi-dimensional case, the a will be replace by $w_i$, and the formular is same as the one dimensional case.</p>\n<p>$$ h = \\sum_{i=1}^N (w_i * x_i) + b $$</p>\n<h4 id=\"Normal-Distribution\"><a href=\"#Normal-Distribution\" class=\"headerlink\" title=\"Normal Distribution\"></a>Normal Distribution</h4><p>Input <strong>X</strong> change to <strong>X_norm</strong><br>Input <strong>Y</strong> change to <strong>y_norm</strong></p>\n<p>For each feature of X input, We have to calculate <strong>mean</strong> and <strong>STD</strong> (Standard Deviation). And Using those two value to get <strong>X_norm</strong> and <strong>y_norm</strong>.</p>\n<p>From real value change to normal distribution:</p>\n<p>$$ X\\_NORM = \\frac{(X - \\overline X)}{STD} $$<br>$$ y\\_NORM = \\frac{(y - \\overline y)}{STD} $$</p>\n<p>Becuase we using normal distribution to get <strong>weight</strong> therefore, when we get back y predict value we need using <strong>mean</strong> and <strong>STD</strong> to change back real value:</p>\n<p>$$ y = y\\_Predict \\cdot y\\_STD + \\overline y $$</p>\n<h4 id=\"Batch\"><a href=\"#Batch\" class=\"headerlink\" title=\"Batch\"></a>Batch</h4><p>With batch <strong>stochastic gradient descent</strong>, we’d need fewer iterations and improve the <strong>LOSS</strong> value faster. </p>\n<p>Think of iteration as epoch - you pass over the entire dataset. So if you split into batches, you’d go over all batches and this will be one iteration.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">batch_size = <span class=\"number\">4</span></span><br><span class=\"line\">batches = int( X.shape[<span class=\"number\">0</span>] / self.batch_size )</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(batches):</span><br><span class=\"line\">\tstart = i * batch_size</span><br><span class=\"line\">\tend = start + batch_size</span><br><span class=\"line\">\tX = X[start:end]</span><br><span class=\"line\">\ty = y[start:end]</span><br></pre></td></tr></table></figure>\n"},{"title":"Logistic Regression","date":"2019-04-30T04:54:41.000Z","_content":"\n#### Sigmoid Function\n0, if $\\sum w_i x_i \\le 0$. For example: $\\sigma(-10) = 0$\n1, if $\\sum w_i x_i \\gt 0$. For example: $\\sigma(10) = 1$\n$$\\sigma(z) = \\frac{1}{1+e^{-z}} $$\n<center><img src=\"{% asset_path SigmoidFunction.png %}\"  width=\"50%\"></center>\n\n<hr>\n\n#### Loss Function\nL(a,b) find a, b. And minimise the loss function.\n\n$$ L(a,b) = -y_i log h_i - (1-y_i) log(h-h_i)  $$\n\nIf $y_i = 0$, then  $L = -1-log(1-h_i) $\nIf $y_i = 1$, then  $L = -log h_i $\n<hr>\n\n#### One dimensional Case\nPassing the linear regression with sigma $(\\sigma)$ function will be logistic regression.\n$$ h = \\sigma (ax+b) $$\n\na = $ a - \\frac{\\partial L}{\\partial a} * \\alpha $\n\nb = $ b - \\frac{\\partial L}{\\partial b} * \\alpha $\n\n$$ \\frac{\\partial L}{\\partial a} = \\frac{1}{N} \\sum_{i=1}^N (h_i - y_i) * x_i $$\n\n$$ \\frac{\\partial L}{\\partial b} = \\frac{1}{N} \\sum_{i=1}^N (h_i - y_i) $$\n<hr>\n\n#### Multi-Dimensional Case\n\n$$ h = \\sigma (\\sum_{i=1}^N (w_i * x_i) + b) $$\n<hr>\n\n#### Normal Distribution\nLogistic will not need apply normal distribution for out range value. \n\nFor more info about using [Normal Distribution](../linear-regression/#Normal-Distribution).\n<hr>\n\n#### Batch\nThe way of Logistic Regression using **Batch** is same way as Linear Regression to use **Batch**.\n\nFor more info about using [Batch](../linear-regression/#Batch)\n<hr>\n\n#### Sigmoid Derivative\n\n$$ \\frac{d}{dx}\\sigma(x) = \\sigma(x) * (1 - \\sigma(x)) $$\n\n<center><img src=\"{% asset_path SigmoidDerivative.png %}\"  width=\"50%\"></center>\n\n{% codeblock lang:python %}\ndef sigmoid_derivative(x):\n    return sigmoid(x) * (1 - sigmoid(x))\n{% endcodeblock %}\n<hr>\n\n#### Precision & Recall\n\nPrecision = True positive / No. of predicted positive \nRecall = True positive / No. of actual positive\nF1_score = $2\\frac{PR}{P+R}$","source":"_posts/logistic-regression.md","raw":"---\ntitle: Logistic Regression\ncategories: Machine Learning\ndate: 2019-04-29 21:54:41\n---\n\n#### Sigmoid Function\n0, if $\\sum w_i x_i \\le 0$. For example: $\\sigma(-10) = 0$\n1, if $\\sum w_i x_i \\gt 0$. For example: $\\sigma(10) = 1$\n$$\\sigma(z) = \\frac{1}{1+e^{-z}} $$\n<center><img src=\"{% asset_path SigmoidFunction.png %}\"  width=\"50%\"></center>\n\n<hr>\n\n#### Loss Function\nL(a,b) find a, b. And minimise the loss function.\n\n$$ L(a,b) = -y_i log h_i - (1-y_i) log(h-h_i)  $$\n\nIf $y_i = 0$, then  $L = -1-log(1-h_i) $\nIf $y_i = 1$, then  $L = -log h_i $\n<hr>\n\n#### One dimensional Case\nPassing the linear regression with sigma $(\\sigma)$ function will be logistic regression.\n$$ h = \\sigma (ax+b) $$\n\na = $ a - \\frac{\\partial L}{\\partial a} * \\alpha $\n\nb = $ b - \\frac{\\partial L}{\\partial b} * \\alpha $\n\n$$ \\frac{\\partial L}{\\partial a} = \\frac{1}{N} \\sum_{i=1}^N (h_i - y_i) * x_i $$\n\n$$ \\frac{\\partial L}{\\partial b} = \\frac{1}{N} \\sum_{i=1}^N (h_i - y_i) $$\n<hr>\n\n#### Multi-Dimensional Case\n\n$$ h = \\sigma (\\sum_{i=1}^N (w_i * x_i) + b) $$\n<hr>\n\n#### Normal Distribution\nLogistic will not need apply normal distribution for out range value. \n\nFor more info about using [Normal Distribution](../linear-regression/#Normal-Distribution).\n<hr>\n\n#### Batch\nThe way of Logistic Regression using **Batch** is same way as Linear Regression to use **Batch**.\n\nFor more info about using [Batch](../linear-regression/#Batch)\n<hr>\n\n#### Sigmoid Derivative\n\n$$ \\frac{d}{dx}\\sigma(x) = \\sigma(x) * (1 - \\sigma(x)) $$\n\n<center><img src=\"{% asset_path SigmoidDerivative.png %}\"  width=\"50%\"></center>\n\n{% codeblock lang:python %}\ndef sigmoid_derivative(x):\n    return sigmoid(x) * (1 - sigmoid(x))\n{% endcodeblock %}\n<hr>\n\n#### Precision & Recall\n\nPrecision = True positive / No. of predicted positive \nRecall = True positive / No. of actual positive\nF1_score = $2\\frac{PR}{P+R}$","slug":"logistic-regression","published":1,"updated":"2020-08-14T18:14:41.877Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckduknz5a0009sniyv0k67gdl","content":"<h4 id=\"Sigmoid-Function\"><a href=\"#Sigmoid-Function\" class=\"headerlink\" title=\"Sigmoid Function\"></a>Sigmoid Function</h4><p>0, if $\\sum w_i x_i \\le 0$. For example: $\\sigma(-10) = 0$<br>1, if $\\sum w_i x_i \\gt 0$. For example: $\\sigma(10) = 1$<br>$$\\sigma(z) = \\frac{1}{1+e^{-z}} $$</p>\n<center><img src=\"/Machine-Learning/logistic-regression/SigmoidFunction.png\" width=\"50%\"></center>\n\n<hr>\n\n<h4 id=\"Loss-Function\"><a href=\"#Loss-Function\" class=\"headerlink\" title=\"Loss Function\"></a>Loss Function</h4><p>L(a,b) find a, b. And minimise the loss function.</p>\n<p>$$ L(a,b) = -y_i log h_i - (1-y_i) log(h-h_i)  $$</p>\n<p>If $y_i = 0$, then  $L = -1-log(1-h_i) $<br>If $y_i = 1$, then  $L = -log h_i $</p>\n<hr>\n\n<h4 id=\"One-dimensional-Case\"><a href=\"#One-dimensional-Case\" class=\"headerlink\" title=\"One dimensional Case\"></a>One dimensional Case</h4><p>Passing the linear regression with sigma $(\\sigma)$ function will be logistic regression.<br>$$ h = \\sigma (ax+b) $$</p>\n<p>a = $ a - \\frac{\\partial L}{\\partial a} * \\alpha $</p>\n<p>b = $ b - \\frac{\\partial L}{\\partial b} * \\alpha $</p>\n<p>$$ \\frac{\\partial L}{\\partial a} = \\frac{1}{N} \\sum_{i=1}^N (h_i - y_i) * x_i $$</p>\n<p>$$ \\frac{\\partial L}{\\partial b} = \\frac{1}{N} \\sum_{i=1}^N (h_i - y_i) $$</p>\n<hr>\n\n<h4 id=\"Multi-Dimensional-Case\"><a href=\"#Multi-Dimensional-Case\" class=\"headerlink\" title=\"Multi-Dimensional Case\"></a>Multi-Dimensional Case</h4><p>$$ h = \\sigma (\\sum_{i=1}^N (w_i * x_i) + b) $$</p>\n<hr>\n\n<h4 id=\"Normal-Distribution\"><a href=\"#Normal-Distribution\" class=\"headerlink\" title=\"Normal Distribution\"></a>Normal Distribution</h4><p>Logistic will not need apply normal distribution for out range value. </p>\n<p>For more info about using <a href=\"../linear-regression/#Normal-Distribution\">Normal Distribution</a>.</p>\n<hr>\n\n<h4 id=\"Batch\"><a href=\"#Batch\" class=\"headerlink\" title=\"Batch\"></a>Batch</h4><p>The way of Logistic Regression using <strong>Batch</strong> is same way as Linear Regression to use <strong>Batch</strong>.</p>\n<p>For more info about using <a href=\"../linear-regression/#Batch\">Batch</a></p>\n<hr>\n\n<h4 id=\"Sigmoid-Derivative\"><a href=\"#Sigmoid-Derivative\" class=\"headerlink\" title=\"Sigmoid Derivative\"></a>Sigmoid Derivative</h4><p>$$ \\frac{d}{dx}\\sigma(x) = \\sigma(x) * (1 - \\sigma(x)) $$</p>\n<center><img src=\"/Machine-Learning/logistic-regression/SigmoidDerivative.png\" width=\"50%\"></center>\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sigmoid_derivative</span><span class=\"params\">(x)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> sigmoid(x) * (<span class=\"number\">1</span> - sigmoid(x))</span><br></pre></td></tr></table></figure>\n<hr>\n\n<h4 id=\"Precision-amp-Recall\"><a href=\"#Precision-amp-Recall\" class=\"headerlink\" title=\"Precision &amp; Recall\"></a>Precision &amp; Recall</h4><p>Precision = True positive / No. of predicted positive<br>Recall = True positive / No. of actual positive<br>F1_score = $2\\frac{PR}{P+R}$</p>\n","site":{"data":{}},"excerpt":"","more":"<h4 id=\"Sigmoid-Function\"><a href=\"#Sigmoid-Function\" class=\"headerlink\" title=\"Sigmoid Function\"></a>Sigmoid Function</h4><p>0, if $\\sum w_i x_i \\le 0$. For example: $\\sigma(-10) = 0$<br>1, if $\\sum w_i x_i \\gt 0$. For example: $\\sigma(10) = 1$<br>$$\\sigma(z) = \\frac{1}{1+e^{-z}} $$</p>\n<center><img src=\"/Machine-Learning/logistic-regression/SigmoidFunction.png\" width=\"50%\"></center>\n\n<hr>\n\n<h4 id=\"Loss-Function\"><a href=\"#Loss-Function\" class=\"headerlink\" title=\"Loss Function\"></a>Loss Function</h4><p>L(a,b) find a, b. And minimise the loss function.</p>\n<p>$$ L(a,b) = -y_i log h_i - (1-y_i) log(h-h_i)  $$</p>\n<p>If $y_i = 0$, then  $L = -1-log(1-h_i) $<br>If $y_i = 1$, then  $L = -log h_i $</p>\n<hr>\n\n<h4 id=\"One-dimensional-Case\"><a href=\"#One-dimensional-Case\" class=\"headerlink\" title=\"One dimensional Case\"></a>One dimensional Case</h4><p>Passing the linear regression with sigma $(\\sigma)$ function will be logistic regression.<br>$$ h = \\sigma (ax+b) $$</p>\n<p>a = $ a - \\frac{\\partial L}{\\partial a} * \\alpha $</p>\n<p>b = $ b - \\frac{\\partial L}{\\partial b} * \\alpha $</p>\n<p>$$ \\frac{\\partial L}{\\partial a} = \\frac{1}{N} \\sum_{i=1}^N (h_i - y_i) * x_i $$</p>\n<p>$$ \\frac{\\partial L}{\\partial b} = \\frac{1}{N} \\sum_{i=1}^N (h_i - y_i) $$</p>\n<hr>\n\n<h4 id=\"Multi-Dimensional-Case\"><a href=\"#Multi-Dimensional-Case\" class=\"headerlink\" title=\"Multi-Dimensional Case\"></a>Multi-Dimensional Case</h4><p>$$ h = \\sigma (\\sum_{i=1}^N (w_i * x_i) + b) $$</p>\n<hr>\n\n<h4 id=\"Normal-Distribution\"><a href=\"#Normal-Distribution\" class=\"headerlink\" title=\"Normal Distribution\"></a>Normal Distribution</h4><p>Logistic will not need apply normal distribution for out range value. </p>\n<p>For more info about using <a href=\"../linear-regression/#Normal-Distribution\">Normal Distribution</a>.</p>\n<hr>\n\n<h4 id=\"Batch\"><a href=\"#Batch\" class=\"headerlink\" title=\"Batch\"></a>Batch</h4><p>The way of Logistic Regression using <strong>Batch</strong> is same way as Linear Regression to use <strong>Batch</strong>.</p>\n<p>For more info about using <a href=\"../linear-regression/#Batch\">Batch</a></p>\n<hr>\n\n<h4 id=\"Sigmoid-Derivative\"><a href=\"#Sigmoid-Derivative\" class=\"headerlink\" title=\"Sigmoid Derivative\"></a>Sigmoid Derivative</h4><p>$$ \\frac{d}{dx}\\sigma(x) = \\sigma(x) * (1 - \\sigma(x)) $$</p>\n<center><img src=\"/Machine-Learning/logistic-regression/SigmoidDerivative.png\" width=\"50%\"></center>\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sigmoid_derivative</span><span class=\"params\">(x)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> sigmoid(x) * (<span class=\"number\">1</span> - sigmoid(x))</span><br></pre></td></tr></table></figure>\n<hr>\n\n<h4 id=\"Precision-amp-Recall\"><a href=\"#Precision-amp-Recall\" class=\"headerlink\" title=\"Precision &amp; Recall\"></a>Precision &amp; Recall</h4><p>Precision = True positive / No. of predicted positive<br>Recall = True positive / No. of actual positive<br>F1_score = $2\\frac{PR}{P+R}$</p>\n"},{"title":"Neural Network","date":"2019-04-15T04:06:39.000Z","_content":"### Boolean Funcitons\n\nArtificial Neuron will have X for input and y for output, the output y from all of x value multiply the weight and plus the bias with bias weight. \n- $x$: x value, $w$: weight, \n\n $$ y = \\sum_{i=1}^N (x_i * w_i) + (bias)(bias_{weight}) $$\n\n<center><img src=\"{% asset_path ArtificialNeuralNetwork.png %}\"  width=\"50%\"></center>\n<hr>\n<center><h4>OR</h4></center>\n\n|  x1 |  x2 |  y  |\n| :-: | :-: | :-: |\n|  0  |  0  |  0  |\n|  0  |  1  |  1  |\n|  1  |  0  |  1  |\n|  1  |  1  |  1  |\n\n{% codeblock lang:python %}\nclass create_OR:\n\n    def forward(self, X):\n        x1 = X[0]\n        x2 = X[1]\n        x1_WEIGHT = 1\n        x2_WEIGHT = 1\n        CONSTANCE = 1\n        CONSTANCE_WEIGHT = 0\n        result = x1 * x1_WEIGHT + x2 * x2_WEIGHT + CONSTANCE * CONSTANCE_WEIGHT\n\n        return {True: 1, False: 0} [result > 0]\n{% endcodeblock %}\n\n<center><h4>AND</h4></center>\n\n|  x1 |  x2 |  y  |\n| :-: | :-: | :-: |\n|  0  |  0  |  0  |\n|  0  |  1  |  0  |\n|  1  |  0  |  0  |\n|  1  |  1  |  1  |\n\n{% codeblock lang:python %}\nclass create_AND:\n    \n    def forward(self, X):\n        x1 = X[0]\n        x2 = X[1]\n        x1_WEIGHT = 1\n        x2_WEIGHT = 1\n        CONSTANCE = 1\n        CONSTANCE_WEIGHT = -1\n        result = x1 * x1_WEIGHT + x2 * x2_WEIGHT + CONSTANCE * CONSTANCE_WEIGHT\n\n        return {True: 1, False: 0} [result > 0]\n{% endcodeblock %}\n\n<center><h4>NOT</h4></center>\n\n|  x  |  y  |\n| :-: | :-: |\n|  0  |  1  |\n|  1  |  0  |\n\n{% codeblock lang:python %}\nclass create_NOT:\n\n    def forward(self, X):\n        result = X\n\n        return {True: 1, False: 0} [result == 0]\n{% endcodeblock %}\n\n<center><h4>XNOR</h4></center>\n\n|  x1 |  x2 |  y  |\n| :-: | :-: | :-: |\n|  0  |  0  |  1  |\n|  0  |  1  |  0  |\n|  1  |  0  |  0  |\n|  1  |  1  |  1  |\n\n{% codeblock lang:python %}\nclass create_XNOR:\n\n    def forward(self, X):\n        x1 = X[0]\n        x2 = X[1]\n\n        xAND = create_AND().forward([x1, x2])\n        xNotAND = create_AND().forward([create_NOT().forward(x1), create_NOT().forward(x2)])\n        xOR = create_OR().forward([xAND, xNotAND])\n\n        return xOR\n{% endcodeblock %}\n\n<center><h4>XOR</h4></center>\n\n|  x1 |  x2 |  y  |\n| :-: | :-: | :-: |\n|  0  |  0  |  0  |\n|  0  |  1  |  1  |\n|  1  |  0  |  1  |\n|  1  |  1  |  0  |\n\n{% codeblock lang:python %}\nclass create_XOR:\n\n    def forward(self, X):\n        XNOR = create_XNOR().forward(X)\n        XOR = create_NOT().forward(XNOR)\n        \n        return XOR\n{% endcodeblock %}","source":"_posts/neural-network.md","raw":"---\ntitle: Neural Network\ncategories: Machine Learning\ndate: 2019-04-14 21:06:39\n---\n### Boolean Funcitons\n\nArtificial Neuron will have X for input and y for output, the output y from all of x value multiply the weight and plus the bias with bias weight. \n- $x$: x value, $w$: weight, \n\n $$ y = \\sum_{i=1}^N (x_i * w_i) + (bias)(bias_{weight}) $$\n\n<center><img src=\"{% asset_path ArtificialNeuralNetwork.png %}\"  width=\"50%\"></center>\n<hr>\n<center><h4>OR</h4></center>\n\n|  x1 |  x2 |  y  |\n| :-: | :-: | :-: |\n|  0  |  0  |  0  |\n|  0  |  1  |  1  |\n|  1  |  0  |  1  |\n|  1  |  1  |  1  |\n\n{% codeblock lang:python %}\nclass create_OR:\n\n    def forward(self, X):\n        x1 = X[0]\n        x2 = X[1]\n        x1_WEIGHT = 1\n        x2_WEIGHT = 1\n        CONSTANCE = 1\n        CONSTANCE_WEIGHT = 0\n        result = x1 * x1_WEIGHT + x2 * x2_WEIGHT + CONSTANCE * CONSTANCE_WEIGHT\n\n        return {True: 1, False: 0} [result > 0]\n{% endcodeblock %}\n\n<center><h4>AND</h4></center>\n\n|  x1 |  x2 |  y  |\n| :-: | :-: | :-: |\n|  0  |  0  |  0  |\n|  0  |  1  |  0  |\n|  1  |  0  |  0  |\n|  1  |  1  |  1  |\n\n{% codeblock lang:python %}\nclass create_AND:\n    \n    def forward(self, X):\n        x1 = X[0]\n        x2 = X[1]\n        x1_WEIGHT = 1\n        x2_WEIGHT = 1\n        CONSTANCE = 1\n        CONSTANCE_WEIGHT = -1\n        result = x1 * x1_WEIGHT + x2 * x2_WEIGHT + CONSTANCE * CONSTANCE_WEIGHT\n\n        return {True: 1, False: 0} [result > 0]\n{% endcodeblock %}\n\n<center><h4>NOT</h4></center>\n\n|  x  |  y  |\n| :-: | :-: |\n|  0  |  1  |\n|  1  |  0  |\n\n{% codeblock lang:python %}\nclass create_NOT:\n\n    def forward(self, X):\n        result = X\n\n        return {True: 1, False: 0} [result == 0]\n{% endcodeblock %}\n\n<center><h4>XNOR</h4></center>\n\n|  x1 |  x2 |  y  |\n| :-: | :-: | :-: |\n|  0  |  0  |  1  |\n|  0  |  1  |  0  |\n|  1  |  0  |  0  |\n|  1  |  1  |  1  |\n\n{% codeblock lang:python %}\nclass create_XNOR:\n\n    def forward(self, X):\n        x1 = X[0]\n        x2 = X[1]\n\n        xAND = create_AND().forward([x1, x2])\n        xNotAND = create_AND().forward([create_NOT().forward(x1), create_NOT().forward(x2)])\n        xOR = create_OR().forward([xAND, xNotAND])\n\n        return xOR\n{% endcodeblock %}\n\n<center><h4>XOR</h4></center>\n\n|  x1 |  x2 |  y  |\n| :-: | :-: | :-: |\n|  0  |  0  |  0  |\n|  0  |  1  |  1  |\n|  1  |  0  |  1  |\n|  1  |  1  |  0  |\n\n{% codeblock lang:python %}\nclass create_XOR:\n\n    def forward(self, X):\n        XNOR = create_XNOR().forward(X)\n        XOR = create_NOT().forward(XNOR)\n        \n        return XOR\n{% endcodeblock %}","slug":"neural-network","published":1,"updated":"2020-08-14T18:14:41.878Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckduknz5b000asniydgdah3qu","content":"<h3 id=\"Boolean-Funcitons\"><a href=\"#Boolean-Funcitons\" class=\"headerlink\" title=\"Boolean Funcitons\"></a>Boolean Funcitons</h3><p>Artificial Neuron will have X for input and y for output, the output y from all of x value multiply the weight and plus the bias with bias weight. </p>\n<ul>\n<li><p>$x$: x value, $w$: weight, </p>\n<p>$$ y = \\sum_{i=1}^N (x_i * w_i) + (bias)(bias_{weight}) $$</p>\n</li>\n</ul>\n<center><img src=\"/Machine-Learning/neural-network/ArtificialNeuralNetwork.png\" width=\"50%\"></center><br><hr><br><center><h4>OR</h4></center>\n\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">x1</th>\n<th style=\"text-align:center\">x2</th>\n<th style=\"text-align:center\">y</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">0</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">1</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">1</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">1</td>\n</tr>\n</tbody>\n</table>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">create_OR</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, X)</span>:</span></span><br><span class=\"line\">        x1 = X[<span class=\"number\">0</span>]</span><br><span class=\"line\">        x2 = X[<span class=\"number\">1</span>]</span><br><span class=\"line\">        x1_WEIGHT = <span class=\"number\">1</span></span><br><span class=\"line\">        x2_WEIGHT = <span class=\"number\">1</span></span><br><span class=\"line\">        CONSTANCE = <span class=\"number\">1</span></span><br><span class=\"line\">        CONSTANCE_WEIGHT = <span class=\"number\">0</span></span><br><span class=\"line\">        result = x1 * x1_WEIGHT + x2 * x2_WEIGHT + CONSTANCE * CONSTANCE_WEIGHT</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> &#123;<span class=\"literal\">True</span>: <span class=\"number\">1</span>, <span class=\"literal\">False</span>: <span class=\"number\">0</span>&#125; [result &gt; <span class=\"number\">0</span>]</span><br></pre></td></tr></table></figure>\n<center><h4>AND</h4></center>\n\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">x1</th>\n<th style=\"text-align:center\">x2</th>\n<th style=\"text-align:center\">y</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">0</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">0</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">0</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">1</td>\n</tr>\n</tbody>\n</table>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">create_AND</span>:</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, X)</span>:</span></span><br><span class=\"line\">        x1 = X[<span class=\"number\">0</span>]</span><br><span class=\"line\">        x2 = X[<span class=\"number\">1</span>]</span><br><span class=\"line\">        x1_WEIGHT = <span class=\"number\">1</span></span><br><span class=\"line\">        x2_WEIGHT = <span class=\"number\">1</span></span><br><span class=\"line\">        CONSTANCE = <span class=\"number\">1</span></span><br><span class=\"line\">        CONSTANCE_WEIGHT = <span class=\"number\">-1</span></span><br><span class=\"line\">        result = x1 * x1_WEIGHT + x2 * x2_WEIGHT + CONSTANCE * CONSTANCE_WEIGHT</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> &#123;<span class=\"literal\">True</span>: <span class=\"number\">1</span>, <span class=\"literal\">False</span>: <span class=\"number\">0</span>&#125; [result &gt; <span class=\"number\">0</span>]</span><br></pre></td></tr></table></figure>\n<center><h4>NOT</h4></center>\n\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">x</th>\n<th style=\"text-align:center\">y</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">1</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">0</td>\n</tr>\n</tbody>\n</table>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">create_NOT</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, X)</span>:</span></span><br><span class=\"line\">        result = X</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> &#123;<span class=\"literal\">True</span>: <span class=\"number\">1</span>, <span class=\"literal\">False</span>: <span class=\"number\">0</span>&#125; [result == <span class=\"number\">0</span>]</span><br></pre></td></tr></table></figure>\n<center><h4>XNOR</h4></center>\n\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">x1</th>\n<th style=\"text-align:center\">x2</th>\n<th style=\"text-align:center\">y</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">1</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">0</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">0</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">1</td>\n</tr>\n</tbody>\n</table>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">create_XNOR</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, X)</span>:</span></span><br><span class=\"line\">        x1 = X[<span class=\"number\">0</span>]</span><br><span class=\"line\">        x2 = X[<span class=\"number\">1</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">        xAND = create_AND().forward([x1, x2])</span><br><span class=\"line\">        xNotAND = create_AND().forward([create_NOT().forward(x1), create_NOT().forward(x2)])</span><br><span class=\"line\">        xOR = create_OR().forward([xAND, xNotAND])</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> xOR</span><br></pre></td></tr></table></figure>\n<center><h4>XOR</h4></center>\n\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">x1</th>\n<th style=\"text-align:center\">x2</th>\n<th style=\"text-align:center\">y</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">0</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">1</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">1</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">0</td>\n</tr>\n</tbody>\n</table>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">create_XOR</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, X)</span>:</span></span><br><span class=\"line\">        XNOR = create_XNOR().forward(X)</span><br><span class=\"line\">        XOR = create_NOT().forward(XNOR)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"keyword\">return</span> XOR</span><br></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"","more":"<h3 id=\"Boolean-Funcitons\"><a href=\"#Boolean-Funcitons\" class=\"headerlink\" title=\"Boolean Funcitons\"></a>Boolean Funcitons</h3><p>Artificial Neuron will have X for input and y for output, the output y from all of x value multiply the weight and plus the bias with bias weight. </p>\n<ul>\n<li><p>$x$: x value, $w$: weight, </p>\n<p>$$ y = \\sum_{i=1}^N (x_i * w_i) + (bias)(bias_{weight}) $$</p>\n</li>\n</ul>\n<center><img src=\"/Machine-Learning/neural-network/ArtificialNeuralNetwork.png\" width=\"50%\"></center><br><hr><br><center><h4>OR</h4></center>\n\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">x1</th>\n<th style=\"text-align:center\">x2</th>\n<th style=\"text-align:center\">y</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">0</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">1</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">1</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">1</td>\n</tr>\n</tbody>\n</table>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">create_OR</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, X)</span>:</span></span><br><span class=\"line\">        x1 = X[<span class=\"number\">0</span>]</span><br><span class=\"line\">        x2 = X[<span class=\"number\">1</span>]</span><br><span class=\"line\">        x1_WEIGHT = <span class=\"number\">1</span></span><br><span class=\"line\">        x2_WEIGHT = <span class=\"number\">1</span></span><br><span class=\"line\">        CONSTANCE = <span class=\"number\">1</span></span><br><span class=\"line\">        CONSTANCE_WEIGHT = <span class=\"number\">0</span></span><br><span class=\"line\">        result = x1 * x1_WEIGHT + x2 * x2_WEIGHT + CONSTANCE * CONSTANCE_WEIGHT</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> &#123;<span class=\"literal\">True</span>: <span class=\"number\">1</span>, <span class=\"literal\">False</span>: <span class=\"number\">0</span>&#125; [result &gt; <span class=\"number\">0</span>]</span><br></pre></td></tr></table></figure>\n<center><h4>AND</h4></center>\n\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">x1</th>\n<th style=\"text-align:center\">x2</th>\n<th style=\"text-align:center\">y</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">0</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">0</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">0</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">1</td>\n</tr>\n</tbody>\n</table>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">create_AND</span>:</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, X)</span>:</span></span><br><span class=\"line\">        x1 = X[<span class=\"number\">0</span>]</span><br><span class=\"line\">        x2 = X[<span class=\"number\">1</span>]</span><br><span class=\"line\">        x1_WEIGHT = <span class=\"number\">1</span></span><br><span class=\"line\">        x2_WEIGHT = <span class=\"number\">1</span></span><br><span class=\"line\">        CONSTANCE = <span class=\"number\">1</span></span><br><span class=\"line\">        CONSTANCE_WEIGHT = <span class=\"number\">-1</span></span><br><span class=\"line\">        result = x1 * x1_WEIGHT + x2 * x2_WEIGHT + CONSTANCE * CONSTANCE_WEIGHT</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> &#123;<span class=\"literal\">True</span>: <span class=\"number\">1</span>, <span class=\"literal\">False</span>: <span class=\"number\">0</span>&#125; [result &gt; <span class=\"number\">0</span>]</span><br></pre></td></tr></table></figure>\n<center><h4>NOT</h4></center>\n\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">x</th>\n<th style=\"text-align:center\">y</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">1</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">0</td>\n</tr>\n</tbody>\n</table>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">create_NOT</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, X)</span>:</span></span><br><span class=\"line\">        result = X</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> &#123;<span class=\"literal\">True</span>: <span class=\"number\">1</span>, <span class=\"literal\">False</span>: <span class=\"number\">0</span>&#125; [result == <span class=\"number\">0</span>]</span><br></pre></td></tr></table></figure>\n<center><h4>XNOR</h4></center>\n\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">x1</th>\n<th style=\"text-align:center\">x2</th>\n<th style=\"text-align:center\">y</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">1</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">0</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">0</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">1</td>\n</tr>\n</tbody>\n</table>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">create_XNOR</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, X)</span>:</span></span><br><span class=\"line\">        x1 = X[<span class=\"number\">0</span>]</span><br><span class=\"line\">        x2 = X[<span class=\"number\">1</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">        xAND = create_AND().forward([x1, x2])</span><br><span class=\"line\">        xNotAND = create_AND().forward([create_NOT().forward(x1), create_NOT().forward(x2)])</span><br><span class=\"line\">        xOR = create_OR().forward([xAND, xNotAND])</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> xOR</span><br></pre></td></tr></table></figure>\n<center><h4>XOR</h4></center>\n\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">x1</th>\n<th style=\"text-align:center\">x2</th>\n<th style=\"text-align:center\">y</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">0</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">1</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">1</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">0</td>\n</tr>\n</tbody>\n</table>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">create_XOR</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, X)</span>:</span></span><br><span class=\"line\">        XNOR = create_XNOR().forward(X)</span><br><span class=\"line\">        XOR = create_NOT().forward(XNOR)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"keyword\">return</span> XOR</span><br></pre></td></tr></table></figure>"},{"title":"旧歌","date":"2019-04-03T06:57:37.000Z","_content":"\n深夜在图书馆学习\n随机列表响起的钢琴声\n\n那是十多年前的歌\n曾经年少 听的只是旋律\n现今入耳的却是歌词\n\n二十有余的年纪\n走过了人生的三或四分之一\n\n过了不顾一切去爱过一个人的年纪\n却也经历过奋不顾身地去爱一段情\n\n有过别人对我的好\n而我那时却不珍惜\n现在回想并不悔恨\n而是多了几分感叹\n\n年少的岁月里\n有爱对 也有爱错\n这不也就是青春\n\n现如今 越长大 \n越不懂如何去爱\n越不敢大胆去爱\n\n接受着孤独\n习惯着孤独\n融入着孤独\n享受着孤独\n\n{% blockquote 品冠, 我以为 %}\n我以为我的温柔 能给你整个宇宙\n我以为我能全力 填满你感情的缺口\n\n他让你红了眼眶 你却还笑着原谅\n原来你早就想好 你要留在谁的身旁\n{% endblockquote %}","source":"_posts/old-melody.md","raw":"---\ntitle: 旧歌\ncategories: Life\ndate: 2019-04-02 23:57:37\n---\n\n深夜在图书馆学习\n随机列表响起的钢琴声\n\n那是十多年前的歌\n曾经年少 听的只是旋律\n现今入耳的却是歌词\n\n二十有余的年纪\n走过了人生的三或四分之一\n\n过了不顾一切去爱过一个人的年纪\n却也经历过奋不顾身地去爱一段情\n\n有过别人对我的好\n而我那时却不珍惜\n现在回想并不悔恨\n而是多了几分感叹\n\n年少的岁月里\n有爱对 也有爱错\n这不也就是青春\n\n现如今 越长大 \n越不懂如何去爱\n越不敢大胆去爱\n\n接受着孤独\n习惯着孤独\n融入着孤独\n享受着孤独\n\n{% blockquote 品冠, 我以为 %}\n我以为我的温柔 能给你整个宇宙\n我以为我能全力 填满你感情的缺口\n\n他让你红了眼眶 你却还笑着原谅\n原来你早就想好 你要留在谁的身旁\n{% endblockquote %}","slug":"old-melody","published":1,"updated":"2020-08-14T18:14:41.883Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckduknz5d000dsniymiw3cg2b","content":"<p>深夜在图书馆学习<br>随机列表响起的钢琴声</p>\n<p>那是十多年前的歌<br>曾经年少 听的只是旋律<br>现今入耳的却是歌词</p>\n<p>二十有余的年纪<br>走过了人生的三或四分之一</p>\n<p>过了不顾一切去爱过一个人的年纪<br>却也经历过奋不顾身地去爱一段情</p>\n<p>有过别人对我的好<br>而我那时却不珍惜<br>现在回想并不悔恨<br>而是多了几分感叹</p>\n<p>年少的岁月里<br>有爱对 也有爱错<br>这不也就是青春</p>\n<p>现如今 越长大<br>越不懂如何去爱<br>越不敢大胆去爱</p>\n<p>接受着孤独<br>习惯着孤独<br>融入着孤独<br>享受着孤独</p>\n<blockquote><p>我以为我的温柔 能给你整个宇宙<br>我以为我能全力 填满你感情的缺口</p>\n<p>他让你红了眼眶 你却还笑着原谅<br>原来你早就想好 你要留在谁的身旁</p>\n<footer><strong>品冠</strong><cite>我以为</cite></footer></blockquote>","site":{"data":{}},"excerpt":"","more":"<p>深夜在图书馆学习<br>随机列表响起的钢琴声</p>\n<p>那是十多年前的歌<br>曾经年少 听的只是旋律<br>现今入耳的却是歌词</p>\n<p>二十有余的年纪<br>走过了人生的三或四分之一</p>\n<p>过了不顾一切去爱过一个人的年纪<br>却也经历过奋不顾身地去爱一段情</p>\n<p>有过别人对我的好<br>而我那时却不珍惜<br>现在回想并不悔恨<br>而是多了几分感叹</p>\n<p>年少的岁月里<br>有爱对 也有爱错<br>这不也就是青春</p>\n<p>现如今 越长大<br>越不懂如何去爱<br>越不敢大胆去爱</p>\n<p>接受着孤独<br>习惯着孤独<br>融入着孤独<br>享受着孤独</p>\n<blockquote><p>我以为我的温柔 能给你整个宇宙<br>我以为我能全力 填满你感情的缺口</p>\n<p>他让你红了眼眶 你却还笑着原谅<br>原来你早就想好 你要留在谁的身旁</p>\n<footer><strong>品冠</strong><cite>我以为</cite></footer></blockquote>"},{"title":"Notre Dame Cathedral","date":"2019-04-16T03:17:03.000Z","_content":"800多年的文物 始终逃不过一场大火的灾难\n3个月前才见过的巴黎圣母院 今日就被一场火灾吞噬得体无完肤\n这也成为了今天全球都在关注的头条\n\n从拿破仑执政时的时代到雨果笔下的浪漫主义长篇小说\n这座哥特式代表的建筑 见证了一个又一个时代的变迁\n\n今日的巴黎 多少人亲眼目睹了这场大火的蔓延\n多少路人 落下了泪 双手合十为这座建筑而祷告\n\n失去的瞬间才领悟到的痛\n多少人为之惋惜 \n这震撼灵魂的建筑 未曾看过的风景\n还未曾目睹就已化为灰烬\n\n庆幸的是教堂前方的主楼建筑未被摧毁 \n而后方的尖塔楼却被大火烧毁\n看着网络上 塔顶坠落的3秒\n随着尖塔楼的坠落 多少人的心也跟随着痛了一下\n\n人生就是一个未知数 \n有想要做的事就去做\n有想去看的地方就去看\n毕竟一生也就一辈子那么短\n\n<center><img src=\"{% asset_path NotreDameCathedral.jpg %}\"  width=\"50%\"></center>","source":"_posts/notre-dame-cathedral.md","raw":"---\ntitle: Notre Dame Cathedral\ncategories: Life\ndate: 2019-04-15 20:17:03\n---\n800多年的文物 始终逃不过一场大火的灾难\n3个月前才见过的巴黎圣母院 今日就被一场火灾吞噬得体无完肤\n这也成为了今天全球都在关注的头条\n\n从拿破仑执政时的时代到雨果笔下的浪漫主义长篇小说\n这座哥特式代表的建筑 见证了一个又一个时代的变迁\n\n今日的巴黎 多少人亲眼目睹了这场大火的蔓延\n多少路人 落下了泪 双手合十为这座建筑而祷告\n\n失去的瞬间才领悟到的痛\n多少人为之惋惜 \n这震撼灵魂的建筑 未曾看过的风景\n还未曾目睹就已化为灰烬\n\n庆幸的是教堂前方的主楼建筑未被摧毁 \n而后方的尖塔楼却被大火烧毁\n看着网络上 塔顶坠落的3秒\n随着尖塔楼的坠落 多少人的心也跟随着痛了一下\n\n人生就是一个未知数 \n有想要做的事就去做\n有想去看的地方就去看\n毕竟一生也就一辈子那么短\n\n<center><img src=\"{% asset_path NotreDameCathedral.jpg %}\"  width=\"50%\"></center>","slug":"notre-dame-cathedral","published":1,"updated":"2020-08-14T18:14:41.879Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckduknz5f000fsniyz11vmkub","content":"<p>800多年的文物 始终逃不过一场大火的灾难<br>3个月前才见过的巴黎圣母院 今日就被一场火灾吞噬得体无完肤<br>这也成为了今天全球都在关注的头条</p>\n<p>从拿破仑执政时的时代到雨果笔下的浪漫主义长篇小说<br>这座哥特式代表的建筑 见证了一个又一个时代的变迁</p>\n<p>今日的巴黎 多少人亲眼目睹了这场大火的蔓延<br>多少路人 落下了泪 双手合十为这座建筑而祷告</p>\n<p>失去的瞬间才领悟到的痛<br>多少人为之惋惜<br>这震撼灵魂的建筑 未曾看过的风景<br>还未曾目睹就已化为灰烬</p>\n<p>庆幸的是教堂前方的主楼建筑未被摧毁<br>而后方的尖塔楼却被大火烧毁<br>看着网络上 塔顶坠落的3秒<br>随着尖塔楼的坠落 多少人的心也跟随着痛了一下</p>\n<p>人生就是一个未知数<br>有想要做的事就去做<br>有想去看的地方就去看<br>毕竟一生也就一辈子那么短</p>\n<center><img src=\"/Life/notre-dame-cathedral/NotreDameCathedral.jpg\" width=\"50%\"></center>","site":{"data":{}},"excerpt":"","more":"<p>800多年的文物 始终逃不过一场大火的灾难<br>3个月前才见过的巴黎圣母院 今日就被一场火灾吞噬得体无完肤<br>这也成为了今天全球都在关注的头条</p>\n<p>从拿破仑执政时的时代到雨果笔下的浪漫主义长篇小说<br>这座哥特式代表的建筑 见证了一个又一个时代的变迁</p>\n<p>今日的巴黎 多少人亲眼目睹了这场大火的蔓延<br>多少路人 落下了泪 双手合十为这座建筑而祷告</p>\n<p>失去的瞬间才领悟到的痛<br>多少人为之惋惜<br>这震撼灵魂的建筑 未曾看过的风景<br>还未曾目睹就已化为灰烬</p>\n<p>庆幸的是教堂前方的主楼建筑未被摧毁<br>而后方的尖塔楼却被大火烧毁<br>看着网络上 塔顶坠落的3秒<br>随着尖塔楼的坠落 多少人的心也跟随着痛了一下</p>\n<p>人生就是一个未知数<br>有想要做的事就去做<br>有想去看的地方就去看<br>毕竟一生也就一辈子那么短</p>\n<center><img src=\"/Life/notre-dame-cathedral/NotreDameCathedral.jpg\" width=\"50%\"></center>"},{"title":"十年","date":"2020-08-14T18:34:56.000Z","_content":"\n都说时间是最好的催化剂\n可你却未曾和我说\n这催化剂是正向的还是逆向呢\n\n### 说不要\n嘴上的不说 并不代表心里的不要 \n心里的想法 说出口时 或许却变成了另一种味道\n<hr>\n\n### 忘不了\n想把你忘掉 越想忘却但却越铭记\n出现的那一刻 撩动心弦的那一年\n<hr>\n\n### 念不消\n是否曾想忘却 真希望能一直延续\n勉为其难是你一直说的话 又何必一直再纠缠\n<hr>\n\n### 断不掉\n度过的时光 挥霍的岁月\n不可逆转的抉择 岂能再度虚妄\n<hr>\n\n青春逝去 你我不再年少 \n踏过了二十而已 迈入了三十而立  \n你的模样 我的惆怅 \n岁月已在你我脸上刻下了抹不去的斑\n\n少时遥盼的二十五 谈婚论嫁家要顾\n如果当初义无反顾 结局或不一塌糊涂\n你所期盼的未来 却是曾经给不了的承诺\n如若相见易相哭 不如舍我孤自一人独处\n\n{% blockquote %}\n不开始或许是最好的结局\n至少还保持着最美的样子\n{% endblockquote %}","source":"_posts/ten-years.md","raw":"---\ntitle: 十年\ncategories: Life\ndate: 2020-08-14 11:34:56\n---\n\n都说时间是最好的催化剂\n可你却未曾和我说\n这催化剂是正向的还是逆向呢\n\n### 说不要\n嘴上的不说 并不代表心里的不要 \n心里的想法 说出口时 或许却变成了另一种味道\n<hr>\n\n### 忘不了\n想把你忘掉 越想忘却但却越铭记\n出现的那一刻 撩动心弦的那一年\n<hr>\n\n### 念不消\n是否曾想忘却 真希望能一直延续\n勉为其难是你一直说的话 又何必一直再纠缠\n<hr>\n\n### 断不掉\n度过的时光 挥霍的岁月\n不可逆转的抉择 岂能再度虚妄\n<hr>\n\n青春逝去 你我不再年少 \n踏过了二十而已 迈入了三十而立  \n你的模样 我的惆怅 \n岁月已在你我脸上刻下了抹不去的斑\n\n少时遥盼的二十五 谈婚论嫁家要顾\n如果当初义无反顾 结局或不一塌糊涂\n你所期盼的未来 却是曾经给不了的承诺\n如若相见易相哭 不如舍我孤自一人独处\n\n{% blockquote %}\n不开始或许是最好的结局\n至少还保持着最美的样子\n{% endblockquote %}","slug":"ten-years","published":1,"updated":"2020-08-14T21:26:33.784Z","_id":"ckduknz5g000isniyydhz0l9i","comments":1,"layout":"post","photos":[],"link":"","content":"<p>都说时间是最好的催化剂<br>可你却未曾和我说<br>这催化剂是正向的还是逆向呢</p>\n<h3 id=\"说不要\"><a href=\"#说不要\" class=\"headerlink\" title=\"说不要\"></a>说不要</h3><p>嘴上的不说 并不代表心里的不要<br>心里的想法 说出口时 或许却变成了另一种味道</p>\n<hr>\n\n<h3 id=\"忘不了\"><a href=\"#忘不了\" class=\"headerlink\" title=\"忘不了\"></a>忘不了</h3><p>想把你忘掉 越想忘却但却越铭记<br>出现的那一刻 撩动心弦的那一年</p>\n<hr>\n\n<h3 id=\"念不消\"><a href=\"#念不消\" class=\"headerlink\" title=\"念不消\"></a>念不消</h3><p>是否曾想忘却 真希望能一直延续<br>勉为其难是你一直说的话 又何必一直再纠缠</p>\n<hr>\n\n<h3 id=\"断不掉\"><a href=\"#断不掉\" class=\"headerlink\" title=\"断不掉\"></a>断不掉</h3><p>度过的时光 挥霍的岁月<br>不可逆转的抉择 岂能再度虚妄</p>\n<hr>\n\n<p>青春逝去 你我不再年少<br>踏过了二十而已 迈入了三十而立<br>你的模样 我的惆怅<br>岁月已在你我脸上刻下了抹不去的斑</p>\n<p>少时遥盼的二十五 谈婚论嫁家要顾<br>如果当初义无反顾 结局或不一塌糊涂<br>你所期盼的未来 却是曾经给不了的承诺<br>如若相见易相哭 不如舍我孤自一人独处</p>\n<blockquote><p>不开始或许是最好的结局<br>至少还保持着最美的样子</p>\n</blockquote>","site":{"data":{}},"excerpt":"","more":"<p>都说时间是最好的催化剂<br>可你却未曾和我说<br>这催化剂是正向的还是逆向呢</p>\n<h3 id=\"说不要\"><a href=\"#说不要\" class=\"headerlink\" title=\"说不要\"></a>说不要</h3><p>嘴上的不说 并不代表心里的不要<br>心里的想法 说出口时 或许却变成了另一种味道</p>\n<hr>\n\n<h3 id=\"忘不了\"><a href=\"#忘不了\" class=\"headerlink\" title=\"忘不了\"></a>忘不了</h3><p>想把你忘掉 越想忘却但却越铭记<br>出现的那一刻 撩动心弦的那一年</p>\n<hr>\n\n<h3 id=\"念不消\"><a href=\"#念不消\" class=\"headerlink\" title=\"念不消\"></a>念不消</h3><p>是否曾想忘却 真希望能一直延续<br>勉为其难是你一直说的话 又何必一直再纠缠</p>\n<hr>\n\n<h3 id=\"断不掉\"><a href=\"#断不掉\" class=\"headerlink\" title=\"断不掉\"></a>断不掉</h3><p>度过的时光 挥霍的岁月<br>不可逆转的抉择 岂能再度虚妄</p>\n<hr>\n\n<p>青春逝去 你我不再年少<br>踏过了二十而已 迈入了三十而立<br>你的模样 我的惆怅<br>岁月已在你我脸上刻下了抹不去的斑</p>\n<p>少时遥盼的二十五 谈婚论嫁家要顾<br>如果当初义无反顾 结局或不一塌糊涂<br>你所期盼的未来 却是曾经给不了的承诺<br>如若相见易相哭 不如舍我孤自一人独处</p>\n<blockquote><p>不开始或许是最好的结局<br>至少还保持着最美的样子</p>\n</blockquote>"},{"title":"Untitle","date":"2019-04-02T06:51:12.000Z","_content":"\n### 2019 Spring Break\n\n用了春假，研究了一下 [**HEXO**](https://hexo.io/)\n一个星期慢慢建起了这个Blog，也想把生活记录成文字的形式保存起来\n\n{% blockquote Steve Jobs http://htmlimg3.scribdassets.com/4988llwpkwmc54c/images/324-157e00fdae/000.jpg Whole Earth Catalog %}\nStay Hungry\nStay Foolish\n{% endblockquote %}","source":"_posts/untitle.md","raw":"---\ntitle: Untitle\ncategories: Uncategorized\ndate: 2019-04-01 23:51:12\n---\n\n### 2019 Spring Break\n\n用了春假，研究了一下 [**HEXO**](https://hexo.io/)\n一个星期慢慢建起了这个Blog，也想把生活记录成文字的形式保存起来\n\n{% blockquote Steve Jobs http://htmlimg3.scribdassets.com/4988llwpkwmc54c/images/324-157e00fdae/000.jpg Whole Earth Catalog %}\nStay Hungry\nStay Foolish\n{% endblockquote %}","slug":"untitle","published":1,"updated":"2020-08-14T18:14:41.884Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckduknz5h000ksniytcdat9su","content":"<h3 id=\"2019-Spring-Break\"><a href=\"#2019-Spring-Break\" class=\"headerlink\" title=\"2019 Spring Break\"></a>2019 Spring Break</h3><p>用了春假，研究了一下 <a href=\"https://hexo.io/\" target=\"_blank\" rel=\"noopener\"><strong>HEXO</strong></a><br>一个星期慢慢建起了这个Blog，也想把生活记录成文字的形式保存起来</p>\n<blockquote><p>Stay Hungry<br>Stay Foolish</p>\n<footer><strong>Steve Jobs</strong><cite><a href=\"http://htmlimg3.scribdassets.com/4988llwpkwmc54c/images/324-157e00fdae/000.jpg\" target=\"_blank\" rel=\"noopener\">Whole Earth Catalog</a></cite></footer></blockquote>","site":{"data":{}},"excerpt":"","more":"<h3 id=\"2019-Spring-Break\"><a href=\"#2019-Spring-Break\" class=\"headerlink\" title=\"2019 Spring Break\"></a>2019 Spring Break</h3><p>用了春假，研究了一下 <a href=\"https://hexo.io/\" target=\"_blank\" rel=\"noopener\"><strong>HEXO</strong></a><br>一个星期慢慢建起了这个Blog，也想把生活记录成文字的形式保存起来</p>\n<blockquote><p>Stay Hungry<br>Stay Foolish</p>\n<footer><strong>Steve Jobs</strong><cite><a href=\"http://htmlimg3.scribdassets.com/4988llwpkwmc54c/images/324-157e00fdae/000.jpg\" target=\"_blank\" rel=\"noopener\">Whole Earth Catalog</a></cite></footer></blockquote>"},{"title":"Random Forest","date":"2019-04-15T01:19:41.000Z","_content":"\n### Random Forest Regression\n\nRandom Forest Regression is based on [Decision Tree Regression](../decision-tree/). It will build 'n_estimators' of Decision Tree Regression, and using the average each decision tree predict value to improve the accuracy value, and control over-fitting. \n\nMore Info Using Scikit-Learn Built-in library: [Scikit-Learn Random Forest Regression](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)\n\n- n_estimators : integer, optional (default=10)\nThe number of trees in the forest.\n\n- sample_ratio: percentage of total size\nFor some big dataset, it may take long time to build for random forest, therefore we can take some sample of the data to same time for build the random forest.\n\n#### Methods\n- fit(self, X: pd.DataFrame, y: np.array)\t\nBuild a forest of trees from the training set (X, y).\n\n- predict(self, X: pd.DataFrame)    \nPredict regression target for X.\n\n- score(self, X: pd.DataFrame, y: np.array)\nReturns the coefficient of determination $R^2$ of the prediction.\n\n#### Graphic Example \n- X-axis: Number of Random Forest estimators\n- y-axis: The accuracy for predict values.\n\n<img src=\"{% asset_path RandomForestRegressor.png %}\"  width=\"100%\">\n\n{% codeblock lang:python %}\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.tree import DecisionTreeRegressor\n\nclass RandomForestRegressor():\n\n\tdef __init__(self, n_estimators, sample_ratio, random_state=None):\n\t\tself.n_estimators = n_estimators\n\t\tself.sample_ratio = sample_ratio\n\t\tself.random_state = random_state\n\n\tdef fit(self, X: pd.DataFrame, y: np.array):\n\t\tif self.random_state!=None:\n\t\t\tnp.random.seed(self.random_state)\n\n\t\tn=self.n_estimators\n\t\tn_sample = X.shape[0]\n\n\t\tself.trees = []\n\t\tfor i in range(n):\n\t\t\ttree = DecisionTreeRegressor()\n\t\t\tindices = np.random.randint(0, n_sample, int(self.sample_ratio*n_sample))\n\t\t\t_ = tree.fit(X.iloc[indices, :], y[indices])\n\t\t\tself.trees.append(tree)\n\n\tdef predict(self, X: pd.DataFrame):\n\t\tpredictList = []\n\t\tfor t in self.trees:\n\t\t\tpredictList.append(t.predict(X))\n\t\tpredictList = np.array(predictList)\n\t\tpredictSampled = predictList.mean(axis=0)\n\t\treturn predictSampled\n\n\n\tdef score(self, X: pd.DataFrame, y: np.array):\n\t\treturn metrics.rsq(self.predict(X), y)\n{% endcodeblock %}","source":"_posts/random-forest.md","raw":"---\ntitle: Random Forest\ncategories: Machine Learning\ndate: 2019-04-14 18:19:41\n---\n\n### Random Forest Regression\n\nRandom Forest Regression is based on [Decision Tree Regression](../decision-tree/). It will build 'n_estimators' of Decision Tree Regression, and using the average each decision tree predict value to improve the accuracy value, and control over-fitting. \n\nMore Info Using Scikit-Learn Built-in library: [Scikit-Learn Random Forest Regression](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)\n\n- n_estimators : integer, optional (default=10)\nThe number of trees in the forest.\n\n- sample_ratio: percentage of total size\nFor some big dataset, it may take long time to build for random forest, therefore we can take some sample of the data to same time for build the random forest.\n\n#### Methods\n- fit(self, X: pd.DataFrame, y: np.array)\t\nBuild a forest of trees from the training set (X, y).\n\n- predict(self, X: pd.DataFrame)    \nPredict regression target for X.\n\n- score(self, X: pd.DataFrame, y: np.array)\nReturns the coefficient of determination $R^2$ of the prediction.\n\n#### Graphic Example \n- X-axis: Number of Random Forest estimators\n- y-axis: The accuracy for predict values.\n\n<img src=\"{% asset_path RandomForestRegressor.png %}\"  width=\"100%\">\n\n{% codeblock lang:python %}\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.tree import DecisionTreeRegressor\n\nclass RandomForestRegressor():\n\n\tdef __init__(self, n_estimators, sample_ratio, random_state=None):\n\t\tself.n_estimators = n_estimators\n\t\tself.sample_ratio = sample_ratio\n\t\tself.random_state = random_state\n\n\tdef fit(self, X: pd.DataFrame, y: np.array):\n\t\tif self.random_state!=None:\n\t\t\tnp.random.seed(self.random_state)\n\n\t\tn=self.n_estimators\n\t\tn_sample = X.shape[0]\n\n\t\tself.trees = []\n\t\tfor i in range(n):\n\t\t\ttree = DecisionTreeRegressor()\n\t\t\tindices = np.random.randint(0, n_sample, int(self.sample_ratio*n_sample))\n\t\t\t_ = tree.fit(X.iloc[indices, :], y[indices])\n\t\t\tself.trees.append(tree)\n\n\tdef predict(self, X: pd.DataFrame):\n\t\tpredictList = []\n\t\tfor t in self.trees:\n\t\t\tpredictList.append(t.predict(X))\n\t\tpredictList = np.array(predictList)\n\t\tpredictSampled = predictList.mean(axis=0)\n\t\treturn predictSampled\n\n\n\tdef score(self, X: pd.DataFrame, y: np.array):\n\t\treturn metrics.rsq(self.predict(X), y)\n{% endcodeblock %}","slug":"random-forest","published":1,"updated":"2020-08-14T18:14:41.883Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckduknz5i000nsniylovnr3br","content":"<h3 id=\"Random-Forest-Regression\"><a href=\"#Random-Forest-Regression\" class=\"headerlink\" title=\"Random Forest Regression\"></a>Random Forest Regression</h3><p>Random Forest Regression is based on <a href=\"../decision-tree/\">Decision Tree Regression</a>. It will build ‘n_estimators’ of Decision Tree Regression, and using the average each decision tree predict value to improve the accuracy value, and control over-fitting. </p>\n<p>More Info Using Scikit-Learn Built-in library: <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\" target=\"_blank\" rel=\"noopener\">Scikit-Learn Random Forest Regression</a></p>\n<ul>\n<li><p>n_estimators : integer, optional (default=10)<br>The number of trees in the forest.</p>\n</li>\n<li><p>sample_ratio: percentage of total size<br>For some big dataset, it may take long time to build for random forest, therefore we can take some sample of the data to same time for build the random forest.</p>\n</li>\n</ul>\n<h4 id=\"Methods\"><a href=\"#Methods\" class=\"headerlink\" title=\"Methods\"></a>Methods</h4><ul>\n<li><p>fit(self, X: pd.DataFrame, y: np.array)<br>Build a forest of trees from the training set (X, y).</p>\n</li>\n<li><p>predict(self, X: pd.DataFrame)<br>Predict regression target for X.</p>\n</li>\n<li><p>score(self, X: pd.DataFrame, y: np.array)<br>Returns the coefficient of determination $R^2$ of the prediction.</p>\n</li>\n</ul>\n<h4 id=\"Graphic-Example\"><a href=\"#Graphic-Example\" class=\"headerlink\" title=\"Graphic Example\"></a>Graphic Example</h4><ul>\n<li>X-axis: Number of Random Forest estimators</li>\n<li>y-axis: The accuracy for predict values.</li>\n</ul>\n<p><img src=\"/Machine-Learning/random-forest/RandomForestRegressor.png\" width=\"100%\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.tree <span class=\"keyword\">import</span> DecisionTreeRegressor</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">RandomForestRegressor</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, n_estimators, sample_ratio, random_state=None)</span>:</span></span><br><span class=\"line\">\t\tself.n_estimators = n_estimators</span><br><span class=\"line\">\t\tself.sample_ratio = sample_ratio</span><br><span class=\"line\">\t\tself.random_state = random_state</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">fit</span><span class=\"params\">(self, X: pd.DataFrame, y: np.array)</span>:</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> self.random_state!=<span class=\"literal\">None</span>:</span><br><span class=\"line\">\t\t\tnp.random.seed(self.random_state)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tn=self.n_estimators</span><br><span class=\"line\">\t\tn_sample = X.shape[<span class=\"number\">0</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tself.trees = []</span><br><span class=\"line\">\t\t<span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(n):</span><br><span class=\"line\">\t\t\ttree = DecisionTreeRegressor()</span><br><span class=\"line\">\t\t\tindices = np.random.randint(<span class=\"number\">0</span>, n_sample, int(self.sample_ratio*n_sample))</span><br><span class=\"line\">\t\t\t_ = tree.fit(X.iloc[indices, :], y[indices])</span><br><span class=\"line\">\t\t\tself.trees.append(tree)</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">predict</span><span class=\"params\">(self, X: pd.DataFrame)</span>:</span></span><br><span class=\"line\">\t\tpredictList = []</span><br><span class=\"line\">\t\t<span class=\"keyword\">for</span> t <span class=\"keyword\">in</span> self.trees:</span><br><span class=\"line\">\t\t\tpredictList.append(t.predict(X))</span><br><span class=\"line\">\t\tpredictList = np.array(predictList)</span><br><span class=\"line\">\t\tpredictSampled = predictList.mean(axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> predictSampled</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">score</span><span class=\"params\">(self, X: pd.DataFrame, y: np.array)</span>:</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> metrics.rsq(self.predict(X), y)</span><br></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"","more":"<h3 id=\"Random-Forest-Regression\"><a href=\"#Random-Forest-Regression\" class=\"headerlink\" title=\"Random Forest Regression\"></a>Random Forest Regression</h3><p>Random Forest Regression is based on <a href=\"../decision-tree/\">Decision Tree Regression</a>. It will build ‘n_estimators’ of Decision Tree Regression, and using the average each decision tree predict value to improve the accuracy value, and control over-fitting. </p>\n<p>More Info Using Scikit-Learn Built-in library: <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\" target=\"_blank\" rel=\"noopener\">Scikit-Learn Random Forest Regression</a></p>\n<ul>\n<li><p>n_estimators : integer, optional (default=10)<br>The number of trees in the forest.</p>\n</li>\n<li><p>sample_ratio: percentage of total size<br>For some big dataset, it may take long time to build for random forest, therefore we can take some sample of the data to same time for build the random forest.</p>\n</li>\n</ul>\n<h4 id=\"Methods\"><a href=\"#Methods\" class=\"headerlink\" title=\"Methods\"></a>Methods</h4><ul>\n<li><p>fit(self, X: pd.DataFrame, y: np.array)<br>Build a forest of trees from the training set (X, y).</p>\n</li>\n<li><p>predict(self, X: pd.DataFrame)<br>Predict regression target for X.</p>\n</li>\n<li><p>score(self, X: pd.DataFrame, y: np.array)<br>Returns the coefficient of determination $R^2$ of the prediction.</p>\n</li>\n</ul>\n<h4 id=\"Graphic-Example\"><a href=\"#Graphic-Example\" class=\"headerlink\" title=\"Graphic Example\"></a>Graphic Example</h4><ul>\n<li>X-axis: Number of Random Forest estimators</li>\n<li>y-axis: The accuracy for predict values.</li>\n</ul>\n<p><img src=\"/Machine-Learning/random-forest/RandomForestRegressor.png\" width=\"100%\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.tree <span class=\"keyword\">import</span> DecisionTreeRegressor</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">RandomForestRegressor</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, n_estimators, sample_ratio, random_state=None)</span>:</span></span><br><span class=\"line\">\t\tself.n_estimators = n_estimators</span><br><span class=\"line\">\t\tself.sample_ratio = sample_ratio</span><br><span class=\"line\">\t\tself.random_state = random_state</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">fit</span><span class=\"params\">(self, X: pd.DataFrame, y: np.array)</span>:</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> self.random_state!=<span class=\"literal\">None</span>:</span><br><span class=\"line\">\t\t\tnp.random.seed(self.random_state)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tn=self.n_estimators</span><br><span class=\"line\">\t\tn_sample = X.shape[<span class=\"number\">0</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tself.trees = []</span><br><span class=\"line\">\t\t<span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(n):</span><br><span class=\"line\">\t\t\ttree = DecisionTreeRegressor()</span><br><span class=\"line\">\t\t\tindices = np.random.randint(<span class=\"number\">0</span>, n_sample, int(self.sample_ratio*n_sample))</span><br><span class=\"line\">\t\t\t_ = tree.fit(X.iloc[indices, :], y[indices])</span><br><span class=\"line\">\t\t\tself.trees.append(tree)</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">predict</span><span class=\"params\">(self, X: pd.DataFrame)</span>:</span></span><br><span class=\"line\">\t\tpredictList = []</span><br><span class=\"line\">\t\t<span class=\"keyword\">for</span> t <span class=\"keyword\">in</span> self.trees:</span><br><span class=\"line\">\t\t\tpredictList.append(t.predict(X))</span><br><span class=\"line\">\t\tpredictList = np.array(predictList)</span><br><span class=\"line\">\t\tpredictSampled = predictList.mean(axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> predictSampled</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">score</span><span class=\"params\">(self, X: pd.DataFrame, y: np.array)</span>:</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> metrics.rsq(self.predict(X), y)</span><br></pre></td></tr></table></figure>"}],"PostAsset":[{"_id":"source/_posts/decision-tree/DecisionTreeRegressor.png","slug":"DecisionTreeRegressor.png","post":"ckduknz530002sniy3ed3vsdp","modified":0,"renderable":0},{"_id":"source/_posts/neural-network/ArtificialNeuralNetwork.png","slug":"ArtificialNeuralNetwork.png","post":"ckduknz5b000asniydgdah3qu","modified":0,"renderable":0},{"_id":"source/_posts/notre-dame-cathedral/NotreDameCathedral.jpg","slug":"NotreDameCathedral.jpg","post":"ckduknz5f000fsniyz11vmkub","modified":0,"renderable":0},{"_id":"source/_posts/random-forest/RandomForestRegressor.png","slug":"RandomForestRegressor.png","post":"ckduknz5i000nsniylovnr3br","modified":0,"renderable":0},{"_id":"source/_posts/decision-tree/DecisionTreeClassifier.png","slug":"DecisionTreeClassifier.png","post":"ckduknz530002sniy3ed3vsdp","modified":0,"renderable":0},{"_id":"source/_posts/logistic-regression/SigmoidDerivative.png","slug":"SigmoidDerivative.png","post":"ckduknz5a0009sniyv0k67gdl","modified":0,"renderable":0},{"_id":"source/_posts/logistic-regression/SigmoidFunction.png","slug":"SigmoidFunction.png","post":"ckduknz5a0009sniyv0k67gdl","modified":0,"renderable":0},{"_id":"source/_posts/joker/joker-1-smile.jpg","slug":"joker-1-smile.jpg","post":"ckduknz570006sniygvdacswg","modified":0,"renderable":0},{"_id":"source/_posts/joker/joker-2-bus.jpg","slug":"joker-2-bus.jpg","post":"ckduknz570006sniygvdacswg","modified":0,"renderable":0},{"_id":"source/_posts/joker/joker-3-card.png","slug":"joker-3-card.png","post":"ckduknz570006sniygvdacswg","modified":0,"renderable":0},{"_id":"source/_posts/joker/joker-4-stairs.jpg","slug":"joker-4-stairs.jpg","post":"ckduknz570006sniygvdacswg","modified":0,"renderable":0},{"_id":"source/_posts/joker/joker-5-notebook.png","slug":"joker-5-notebook.png","post":"ckduknz570006sniygvdacswg","modified":0,"renderable":0},{"_id":"source/_posts/joker/joker-6-dont-smile.png","slug":"joker-6-dont-smile.png","post":"ckduknz570006sniygvdacswg","modified":0,"renderable":0},{"_id":"source/_posts/joker/joker-7-gun.png","slug":"joker-7-gun.png","post":"ckduknz570006sniygvdacswg","modified":0,"renderable":0},{"_id":"source/_posts/joker/joker-8-blood-smile.png","slug":"joker-8-blood-smile.png","post":"ckduknz570006sniygvdacswg","modified":0,"renderable":0}],"PostCategory":[{"post_id":"ckduknz4y0000sniyby6dqxs8","category_id":"ckduknz550004sniy0pkj6m73","_id":"ckduknz5c000csniy30tcnizh"},{"post_id":"ckduknz580007sniyxnoqub7f","category_id":"ckduknz550004sniy0pkj6m73","_id":"ckduknz5e000esniyrktly1mx"},{"post_id":"ckduknz5a0009sniyv0k67gdl","category_id":"ckduknz550004sniy0pkj6m73","_id":"ckduknz5g000hsniyqcm7jqqr"},{"post_id":"ckduknz530002sniy3ed3vsdp","category_id":"ckduknz550004sniy0pkj6m73","_id":"ckduknz5h000jsniyx3n0bg2h"},{"post_id":"ckduknz5b000asniydgdah3qu","category_id":"ckduknz550004sniy0pkj6m73","_id":"ckduknz5h000lsniyznkhfwha"},{"post_id":"ckduknz560005sniyyjincyr7","category_id":"ckduknz550004sniy0pkj6m73","_id":"ckduknz5i000osniy50ryu0dy"},{"post_id":"ckduknz5g000isniyydhz0l9i","category_id":"ckduknz5f000gsniy3szcyoqu","_id":"ckduknz5j000psniy6p2ahr0c"},{"post_id":"ckduknz570006sniygvdacswg","category_id":"ckduknz5f000gsniy3szcyoqu","_id":"ckduknz5j000rsniy26lieuwh"},{"post_id":"ckduknz5i000nsniylovnr3br","category_id":"ckduknz550004sniy0pkj6m73","_id":"ckduknz5j000ssniyzg5puto9"},{"post_id":"ckduknz5d000dsniymiw3cg2b","category_id":"ckduknz5f000gsniy3szcyoqu","_id":"ckduknz5k000usniyrv4gbq9k"},{"post_id":"ckduknz5f000fsniyz11vmkub","category_id":"ckduknz5f000gsniy3szcyoqu","_id":"ckduknz5k000vsniydqc61ky0"},{"post_id":"ckduknz5h000ksniytcdat9su","category_id":"ckduknz5k000tsniyj2se9wow","_id":"ckduknz5k000wsniyp2ciijrs"}],"PostTag":[],"Tag":[]}}